[
  {
    "path": "posts/2021-08-22-marinas-hw3/",
    "title": "Marina's HW3",
    "description": "Basic data wrangling",
    "author": [
      {
        "name": "Marina",
        "url": {}
      }
    ],
    "date": "2021-08-22",
    "categories": [],
    "contents": "\nThe requirement for HW3 is to read in data and perform basic data wrangling operations.\nI’ll bring in something from the existing _data folder in my fork and hope it works out with the pull request (fingers crossed).\nStep 1: Load libraries and read in a dataset\n\n\nlibrary(dplyr)\nlibrary(tidyr)\ndf <- read.csv(file=\"../../_data/hotel_bookings.csv\")\n\n\n\nStep 2: Take a quick peek at the dataset\n\n\nglimpse(df)\n\n\nRows: 119,390\nColumns: 32\n$ hotel                          <chr> \"Resort Hotel\", \"Resort Hotel…\n$ is_canceled                    <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\n$ lead_time                      <int> 342, 737, 7, 13, 14, 14, 0, 9…\n$ arrival_date_year              <int> 2015, 2015, 2015, 2015, 2015,…\n$ arrival_date_month             <chr> \"July\", \"July\", \"July\", \"July…\n$ arrival_date_week_number       <int> 27, 27, 27, 27, 27, 27, 27, 2…\n$ arrival_date_day_of_month      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ stays_in_weekend_nights        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ stays_in_week_nights           <int> 0, 0, 1, 1, 2, 2, 2, 2, 3, 3,…\n$ adults                         <int> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2,…\n$ children                       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ babies                         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ meal                           <chr> \"BB\", \"BB\", \"BB\", \"BB\", \"BB\",…\n$ country                        <chr> \"PRT\", \"PRT\", \"GBR\", \"GBR\", \"…\n$ market_segment                 <chr> \"Direct\", \"Direct\", \"Direct\",…\n$ distribution_channel           <chr> \"Direct\", \"Direct\", \"Direct\",…\n$ is_repeated_guest              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ previous_cancellations         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ previous_bookings_not_canceled <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ reserved_room_type             <chr> \"C\", \"C\", \"A\", \"A\", \"A\", \"A\",…\n$ assigned_room_type             <chr> \"C\", \"C\", \"C\", \"A\", \"A\", \"A\",…\n$ booking_changes                <int> 3, 4, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ deposit_type                   <chr> \"No Deposit\", \"No Deposit\", \"…\n$ agent                          <chr> \"NULL\", \"NULL\", \"NULL\", \"304\"…\n$ company                        <chr> \"NULL\", \"NULL\", \"NULL\", \"NULL…\n$ days_in_waiting_list           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ customer_type                  <chr> \"Transient\", \"Transient\", \"Tr…\n$ adr                            <dbl> 0.00, 0.00, 75.00, 75.00, 98.…\n$ required_car_parking_spaces    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ total_of_special_requests      <int> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,…\n$ reservation_status             <chr> \"Check-Out\", \"Check-Out\", \"Ch…\n$ reservation_status_date        <chr> \"2015-07-01\", \"2015-07-01\", \"…\n\nStep 3: Apply required functions\n\n\ndf %>%\n  filter(is_canceled == 0) %>% # Exclude canceled bookings\n  select(hotel_type = hotel,\n         arrival_date_year,\n         adults) %>%\n  group_by(hotel_type, arrival_date_year) %>%\n  summarise(mean_adults = mean(adults)) %>%\n  arrange(hotel_type, arrival_date_year) %>%\n  pivot_wider(id_cols = hotel_type,\n              names_from = arrival_date_year,\n              values_from = mean_adults)\n\n\n# A tibble: 2 × 4\n# Groups:   hotel_type [2]\n  hotel_type   `2015` `2016` `2017`\n  <chr>         <dbl>  <dbl>  <dbl>\n1 City Hotel     1.73   1.84   1.86\n2 Resort Hotel   1.86   1.83   1.82\n\nLooking at completed (non-canceled) bookings and splitting them by hotel type, we can see that over time, the mean number of adults checking in to city hotels has slightly increased, while the opposite is true in resort hotels.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-22-marinas-hw4/",
    "title": "Marina's HW4",
    "description": "Getting started with the final project & descriptive statistics",
    "author": [
      {
        "name": "Marina",
        "url": {}
      }
    ],
    "date": "2021-08-22",
    "categories": [],
    "contents": "\nThe requirements for this homework are to:\nSelect at least two variables of interest from the dataset\nProvide a basic description of the variables\nClean and recode as needed\nPresent summary descriptive statistics of the recoded variables\nFor this homework, I’ll be using data published by the federal government on the use of Lifeline. I’ll include a more thorough description of the dataset in my final project.\nStep 1: load libraries\n\n\n#Actually Necessary\n  library(RSocrata)\n  library(tidyverse)\n#To play around with formatting\n  library(knitr) #Trying this out for tables\n  library(kableExtra)\n\n\n\nStep 2: read in the data\nI’ll be bringing in the data using the Socrata Open Data API.\n\n\ndf <- read.socrata(\"https://opendata.usac.org/resource/tfxa-gt3x.json\")\ndim(df)\n\n\n[1] 207288     17\n\nThe dataset is currently huge as it contains multiple years of data for every US state and territory. We will narrow it down to Massachusetts observations starting from 2018.\n\n\ndf <- filter(df, state == \"MA\" & support_year >= 2018)\nglimpse(df)  \n\n\nRows: 906\nColumns: 17\n$ state                          <chr> \"MA\", \"MA\", \"MA\", \"MA\", \"MA\",…\n$ sac                            <chr> \"115112\", \"115112\", \"115112\",…\n$ sac_name                       <chr> \"VERIZON NEW ENGLAND - MA\", \"…\n$ disbursement_year              <chr> \"2018\", \"2018\", \"2018\", \"2018…\n$ disbursement_month             <chr> \"4\", \"2\", \"4\", \"3\", \"6\", \"11\"…\n$ disbursement_month_date_format <dttm> 2018-04-01, 2018-02-01, 2018…\n$ support_year                   <chr> \"2018\", \"2018\", \"2018\", \"2018…\n$ support_month                  <chr> \"3\", \"1\", \"2\", \"2\", \"5\", \"10\"…\n$ support_month_date_format      <dttm> 2018-03-01, 2018-01-01, 2018…\n$ support_type                   <chr> \"TRIBAL\", \"TRIBAL\", \"TRIBAL\",…\n$ provider_type                  <chr> \"ILEC\", \"ILEC\", \"ILEC\", \"ILEC…\n$ technology_type                <chr> \"WIRELINE\", \"WIRELINE\", \"WIRE…\n$ service_type                   <chr> \"VOICE\", \"VOICE\", \"VOICE\", \"V…\n$ submission_type                <chr> \"CLAIM\", \"CLAIM\", \"CLAIM\", \"C…\n$ disbursement_type              <chr> \"LIFELINE\", \"LIFELINE\", \"LIFE…\n$ subscriber_count               <chr> \"1\", \"1\", \"1\", \"8\", \"7\", \"7\",…\n$ disbursement_amount            <chr> \"9\", \"9\", \"9\", \"74\", \"65\", \"6…\n\nStep 3: pick variables to explore and clean\nFor this HW, I’m going to focus on sac_name, disbursement_year, support_year and subscriber_count.\nsac_name refers to the name of the companies offering Lifeline service in Massachusetts.\n\n\n(#tab:Providers table)Companies offering service in Massachusetts each year\n\n\n\n\n2018\n\n\n2019\n\n\n2020\n\n\n2021\n\n\nCITY OF WESTFIELD GAS & ELECTRIC LIGHT DEPARTMENT\n\n\n0\n\n\n0\n\n\n0\n\n\n7\n\n\nGLOBAL CONNECTION INC OF AMERICA - MA\n\n\n24\n\n\n26\n\n\n22\n\n\n8\n\n\nGRANBY TEL. & TELE. CO.-MA\n\n\n22\n\n\n42\n\n\n36\n\n\n16\n\n\nTRACFONE WIRELESS INC. - MA\n\n\n43\n\n\n36\n\n\n52\n\n\n105\n\n\nVERIZON NEW ENGLAND - MA\n\n\n36\n\n\n90\n\n\n57\n\n\n38\n\n\nVIRGIN MOBILE USA, LP - MA\n\n\n33\n\n\n36\n\n\n124\n\n\n53\n\n\nWith the addition of Westfield Gas & Electric, we have a total of 6 companies offering service in the state as of 2021. Because there is no company ID included in the dataset, we need to use their names to identify them.\nHowever, their names have been known to change through time. For clarity and readability, we will be changing their names, building in an assumption that the total number of providers is 6– if the number changes, that means it’s worth looking into the data first before moving forward.\n\n\n#Rename companies, but only if the total number of companies is 6\n  #(otherwise there might have been name changes that I'd have to\n  #account for)\n  if (length(unique(df$sac_name)) == 6) {\n  df <- df %>%\n        mutate(sac_name = str_to_title(sac_name)) %>%\n        mutate(provider = case_when(\n              startsWith(sac_name, \"City\") ~ 'Westfield',\n              startsWith(sac_name, \"Global\") ~ 'StandUp Wireless/Global Connection',\n              startsWith(sac_name, \"Granby\") ~ 'Granby',\n              startsWith(sac_name, \"Tracfone\") ~ 'Tracfone/Safelink',\n              startsWith(sac_name, \"Verizon\") ~ 'Verizon',\n              startsWith(sac_name, \"Virgin\") ~ 'Assurance/Virgin/T-Mobile'\n              ))\n    }  else {\n      print(\"There has been a change in the number of unique companies\")\n    }\n  \n  #Confirm it worked\n  print(unique(df$provider))\n\n\n[1] \"Verizon\"                           \n[2] \"Granby\"                            \n[3] \"Tracfone/Safelink\"                 \n[4] \"StandUp Wireless/Global Connection\"\n[5] \"Assurance/Virgin/T-Mobile\"         \n[6] \"Westfield\"                         \n\nNext, we explore disbursement_year and support_year. support_year represents the year where the company provided service, and for which it’s entitled to receive financial support. disbursement_year represents the year in which said support was actually disbursement and given to the company.\n\n\n xtabs(~ disbursement_year + support_year, df)\n\n\n                 support_year\ndisbursement_year 2018 2019 2020 2021\n             2018  158    0    0    0\n             2019   71  159    0    0\n             2020   76   71  144    0\n             2021  114    6   16   91\n\nSurprisingly, disbursement and support years do not match 1:1. This is because companies make corrections to their claimed support amounts months, and even years, after the fact.\nBecause of this, the number of subscribers that one company claims to have served in a specific time period can end up changing over time. Hence, one specific support period for one specific company may present as more than one observation:\n\n\ndf %>%\n  filter(provider == \"Tracfone/Safelink\" &\n           support_year == \"2019\" &\n           support_month == \"7\") %>%\n  select(provider, \n         disbursement_date = disbursement_month_date_format,\n         support_date = support_month_date_format,\n         service_type,\n         subscriber_count)\n\n\n           provider disbursement_date support_date service_type\n1 Tracfone/Safelink        2020-07-01   2019-07-01        VOICE\n2 Tracfone/Safelink        2019-08-01   2019-07-01        VOICE\n3 Tracfone/Safelink        2019-08-01   2019-07-01    BROADBAND\n4 Tracfone/Safelink        2019-08-01   2019-07-01      BUNDLED\n  subscriber_count\n1               -3\n2            41994\n3                7\n4              113\n\n\n\n\nHere, we see that in August 2019, Tracfone was disbursed funds for 42114 subscribers it claimed to serve in July 2019. A year later, in July 2020, Tracfone submitted a correction of -3 voice subscribers, which would bring the actual total to 42111.\nIn order to tidy up these corrections, we’ll need to flatten the dataset to add up all claims of support across different disbursement dates. But first, let’s try to view the amount of corrections present.\n\n\ndf %>%\n  ggplot(aes(x = support_month_date_format, \n             y = disbursement_month_date_format)) +\n  geom_point() +\n  xlab(\"Support Date\") +\n  ylab(\"Disbursement Date\")\n\n\n\n\nEarlier years have more corrections– it might be worth looking into whether the adjustments follow any patterns.\nStep 4: flatten the dataset\n\n\n  subscribers <- df %>%\n                  filter(!submission_type == \"ADJUSTMENT\") %>% #Exclude fin. adj. rows\n                  select(provider, \n                         support_year,\n                         support_month = support_month_date_format,\n                         technology_type,\n                         service_type,\n                         subscriber_count) %>%\n                  group_by(provider,\n                           support_year,\n                           support_month,\n                           technology_type,\n                           service_type) %>%\n                  summarise(subscriber_count = sum(as.numeric(subscriber_count)))\n\n\n\nBy aggregating subscriber counts across various corrections, we’ve gone from 906 to 562 rows.\nStep 5: present summary descriptives\nLet’s take a look at how many subscribers each company has claimed to serve, on average, each year.\n\n\nsubscribers %>%\n  ungroup() %>%\n  select(provider,\n         support_year,\n         subscriber_count) %>%\n  group_by(provider, support_year) %>%\n  summarise(mean_subscribers = mean(subscriber_count)) %>%\n  mutate(mean_subscribers = round(mean_subscribers, 2)) %>%\n  pivot_wider(id_cols = provider,\n              names_from = support_year,\n              values_from = mean_subscribers) %>%\n    kbl(caption = \"Average subscribers per year\") %>%\n    kable_paper(\"hover\", full_width = F)\n\n\n\nTable 1: Average subscribers per year\n\n\nprovider\n\n\n2018\n\n\n2019\n\n\n2020\n\n\n2021\n\n\nAssurance/Virgin/T-Mobile\n\n\n32405.56\n\n\n23791.56\n\n\n20138.58\n\n\n22147.67\n\n\nGranby\n\n\n4.17\n\n\n2.42\n\n\n1.80\n\n\n2.43\n\n\nStandUp Wireless/Global Connection\n\n\n62.19\n\n\n39.00\n\n\n171.00\n\n\n288.14\n\n\nTracfone/Safelink\n\n\n19570.81\n\n\n14276.61\n\n\n12278.61\n\n\n11346.00\n\n\nVerizon\n\n\n3909.86\n\n\n3340.03\n\n\n3055.31\n\n\n2944.00\n\n\nWestfield\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n29.86\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:23:23-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-20-active-duty-marital-status-hw3-bakharia/",
    "title": "Active Duty Marital Status HW3 - bakharia",
    "description": "This post contains data wrangling of the Active Duty Marital Status and some basic visualization of the output.",
    "author": [
      {
        "name": "Shubham Mishra",
        "url": {}
      }
    ],
    "date": "2021-08-20",
    "categories": [],
    "contents": "\nMILITARY DATASET\nLoading Libraries\n\n\nlibrary(tidyverse)\n\n\n\nDefining the function to read the dataset and return a dataframe based on the name of the sheet in the excel file\n\n\n# SNC -> SINGLE NO CHILD; SC <- SINGLE WITH CHILD; JSM <- JOINT SERVICE MARRIAGE; CVM <- CIVILLIAN MARRIAGE; _M <- MALE; _F <- FEMALE; _T <- TOTAL\ndf <- function(x){\n  temp <- readxl:: read_excel(\"../../_data/ActiveDuty_MaritalStatus.xls\",\n                              sheet = sprintf(\"%s\", x),\n                              col_names = c(\"d\", \"Pay Grade\", \"SNC_M\", \"SNC_F\", \"SNC_T\", \"SC_M\", \"SC_F\", \"SC_T\", \"JSM_M\", \"JSM_F\", \"JSM_T\", \"CVM_M\", \"CVM_F\",                                                                 \"CVM_T\", \"Total_M\", \"Total_F\", \"Total_T\")\n                                               ) %>%\n                              filter(! str_detect(`Pay Grade`,\"TOTAL\")) %>%\n                              select(! starts_with(\"d\")) %>%\n                              cbind(Branch = sprintf(\"%s\", x)\n                              )\n  return (temp)\n}\n\n\n\nReading all the sheets[Airforce, Marine, Navy, Army] from the excel file\n\n\nAirForce_MaritalStatus <- df(\"AirForce\")\n\nMarineCorps_MaritalStatus <- df(\"MarineCorps\")\n  \nNavy_MaritalStatus <- df(\"Navy\")\n\nArmy_MaritalStatus <- df(\"Army\")\n\n\n\n*In the code chunk below, the 4 dataframes are combined by row(as they have the same col names) and then filtered to removed the Total and ’_T’ columns. Using Regex expressions the Pay Grade is broken down into two new columns: Grade and Type. The columns in the combined dataframe are reordered to prepare it for pivot. The pivot_longer function clubs all the columns(SNC, SC, JSM, CVM) and their names and values are stored in Status and Enrolled respectively. The values stored in the Status Columns are them further used to extract gender and store it in a seperate col(Gender). Based on the values in the Status column values are stored in Plan indicating whether the individuals have subscribed to a Single or Family Plan (SNC_M | SNC_F == Single else Family). At the end the Status column is changed to Single or Family depending on the value (SNC or SC == Single)*\n\n\nCombined_MaritalStatus <- dplyr::bind_rows(AirForce_MaritalStatus, MarineCorps_MaritalStatus, Navy_MaritalStatus, Army_MaritalStatus) %>%\n  arrange(`Pay Grade`) %>%\n  select(! contains(\"_T\")) %>%\n  mutate(Grade = str_extract(`Pay Grade`, '[^-]*$'), Type = str_extract(`Pay Grade`, '^[^-]*[^ -]')) %>%\n  select(! contains(\"Pay\")) %>%\n  select(! contains(\"Total\"))\nCombined_MaritalStatus <- Combined_MaritalStatus[,c(\"Type\", \"Grade\", \"Branch\", \"SNC_M\", \"SNC_F\", \"SC_M\", \"SC_F\", \"JSM_M\", \"JSM_F\", \"CVM_M\", \"CVM_F\")] %>% #Reordering\n## pivot (newer functions longer and wider), name_palette\n  pivot_longer(SNC_M:CVM_F, values_to = \"Enrolled\", names_to = \"Status\") %>%\n  mutate(\n    Plan = ifelse(as.character(Status) == \"SNC_M\" | as.character(Status) == \"SNC_F\" , \"Single\", \"Family\"), \n    Gender = str_extract(as.character(Status), \"[^_]*$\"),\n    Status = ifelse(str_detect(as.character(Status), \"S\"), \"Single\", \"Married\"),\n    Enrolled = as.numeric(Enrolled)\n    ) %>%\n  filter(! Enrolled == 0)\n\n\n\nDepicts the number of males and females in all the categories\n\n\nCombined_MaritalStatus %>%\n  #mutate(Enrolled = log2(Enrolled)) %>%\n  ggplot(aes(x = Gender, y = Enrolled, fill = Gender)) + \n  geom_bar(stat = \"identity\")+\n  facet_wrap(vars(Type)) +\n  scale_fill_hue(c = 40) +\n  labs(title = \"Type vs Gender Enrolled\") +\n  theme_minimal()\n\n\n\n\nIt is observed that in the female category that there are more Single Parents\n\n\nCombined_MaritalStatus %>%\n  #mutate(Enrolled = log2(Enrolled)) %>%\n  ggplot(aes(x = Gender, y = Enrolled, fill = Gender)) + \n  geom_bar(stat = \"identity\")+\n  facet_wrap(vars(Status)) +\n  scale_fill_hue(c = 40) +\n  labs(title = \"Type vs Marital Status Enrolled\") +\n  theme_minimal()\n\n\n\nCombined_MaritalStatus %>%\n  #mutate(Enrolled = log2(Enrolled)) %>%\n  ggplot(aes(x = Gender, y = Enrolled, fill = Gender)) + \n  geom_bar(stat = \"identity\")+\n  facet_wrap(vars(Plan)) +\n  scale_fill_hue(c = 40) +\n  labs(title = \"Type vs Status Enrolled\") +\n  theme_minimal()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-20-active-duty-marital-status-hw3-bakharia/active-duty-marital-status-hw3-bakharia_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-08-24T08:22:25-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-20-active-duty-marital-status/",
    "title": "Active Duty Marital Status",
    "description": "A short description of the post.",
    "author": [],
    "date": "2021-08-20",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:27-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-20-homework3-functions-by-mm/",
    "title": "Homework3: Functions by MM",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Michelle Manning",
        "url": {}
      }
    ],
    "date": "2021-08-20",
    "categories": [],
    "contents": "\nFirst, I loaded in the packages and files I needed.I am using the poultry data set for this project.\n\n\nlibrary(blogbuilder)\nlibrary(distill)\nlibrary(tidyverse)\npoultry <- read.csv(file=\"../../_data/poultry_tidy.csv\")\nlibrary(usethis)\n\n\n\nI first used the functions select, filter, and arrange. I saved sumarize for when I ran the function group_by to highlight the function’s use. After those, I used rename, case_when, across, pivot_longer and wider, purrr, and lapply.\n\n\n#Functions\n#select, filter, arrange\npoultry2 <- poultry %>% \n  select(Product, Year, Price_Dollar) %>% \n  filter(Product == \"Whole\") %>% \n  arrange(Product, Price_Dollar, Year)\nhead(poultry2)  \n\n\n  Product Year Price_Dollar\n1   Whole 2004      1.97500\n2   Whole 2004      1.97500\n3   Whole 2004      2.09000\n4   Whole 2004      2.12000\n5   Whole 2004      2.14500\n6   Whole 2004      2.16375\n\nhead(poultry)\n\n\n  Product Year    Month Price_Dollar\n1   Whole 2013  January        2.385\n2   Whole 2013 February        2.385\n3   Whole 2013    March        2.385\n4   Whole 2013    April        2.385\n5   Whole 2013      May        2.385\n6   Whole 2013     June        2.385\n\n#Optional Functions\n#group_by & summarize\npoultry_grouped <- poultry2 %>% \n  group_by(Year)\nsummarise(poultry_grouped, Year = mean(Year))\n\n\n# A tibble: 10 × 1\n    Year\n   <dbl>\n 1  2004\n 2  2005\n 3  2006\n 4  2007\n 5  2008\n 6  2009\n 7  2010\n 8  2011\n 9  2012\n10  2013\n\n#rename\ncolnames(poultry2)\n\n\n[1] \"Product\"      \"Year\"         \"Price_Dollar\"\n\npoultry3 <- poultry2 %>%\n  rename(Price=Price_Dollar)\n#case_when  \nmean(poultry3$Price)\n\n\n[1] 2.305333\n\npoultry4 <- poultry2 %>%\n  mutate(Above.Below.Avg = case_when(Price_Dollar >= 2.305333 ~\"Above Mean\", TRUE ~ \"Below Mean\"))\n#across\npoultry5 <- poultry2 %>%\n  group_by(Year) %>%\n  summarise(across(starts_with(\"Price_Dollar\"), list(mean = mean, sd = sd)))\n#Pivoting\npoultry_wider <- poultry %>% \n  pivot_wider(names_from = \"Product\",values_from = \"Price_Dollar\")\npoultry_longer <- poultry_wider %>% \n  pivot_longer(Whole:Thighs, names_to = \"Product\", values_to = \"Price_Dollar\")\n#purrr\nlibrary(stringr)\navg_fun <- function(x)\npurrr::map_dbl(poultry3, n_distinct)\n#lapply\npoultry_upper <- lapply(poultry2$Product, toupper)\nstr(poultry_upper)\n\n\nList of 120\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n $ : chr \"WHOLE\"\n  [list output truncated]\n\nI decided to use ggplot to edify my knowledge of it. Here is the poultry’s Product variable’s price plotted by the month and year.\n\n\n#plotting\nggplot(data = poultry, mapping = aes(x=Month, y = Price_Dollar, color = Product, group = Product))+ \n  geom_point()+ \n  geom_line() +\n  facet_grid(~Year)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-20-homework3-functions-by-mm/homework3-functions-by-mm_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-24T08:22:31-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-20-us-homicide-and-execution-data/",
    "title": "U.S. execution data",
    "description": "Importing and describing data for Homework 2.",
    "author": [
      {
        "name": "Leah Dion",
        "url": {}
      }
    ],
    "date": "2021-08-20",
    "categories": [
      ".homework 2 .importing data"
    ],
    "contents": "\nLoading in the tidyverse package:\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n\n\n\nAbout the data\nThis data set is a table of all executions carried out in the United States starting from when the Supreme Court reinstated the death penalty in 1976 until the data was collected in 2016. It was published by the Death Penalty Information Center.\nReading in the file, assigning it to variable exec, and displaying the first ten rows:\n\n\nexec <- read.csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQDMoiJXVldahXzL4S037MGb7DgZMfeqfrR-zYtDJ_U-Sd6FS35W7WU_6N8pXBOm9NOBGIM8BEui37x/pub?gid=0&single=true&output=csv\")\nhead(exec, 10)\n\n\n   Execution. Execution.Date First.Name  Last.Name Middle.Name.s.\n1           1     01/17/1977       Gary    Gilmore           Mark\n2           2     05/25/1979       John Spenkelink               \n3           3     10/22/1979      Jesse     Bishop               \n4           4     03/09/1981     Steven       Judy               \n5           5     08/10/1982      Frank    Coppola               \n6           6     12/07/1982    Charlie     Brooks               \n7           7     04/22/1983       John      Evans               \n8           8     09/02/1983      Jimmy       Gray            Lee\n9           9     11/30/1983     Robert   Sullivan               \n10         10     12/14/1983     Robert   Williams          Wayne\n   Suffix  Race  Sex  Region       State                  County\n1         White Male    West        Utah             Utah County\n2         White Male   South     Florida             Leon County\n3         White Male    West      Nevada            Clark County\n4         White Male Midwest     Indiana           Morgan County\n5         White Male   South    Virginia       Newport News city\n6         Black Male   South       Texas          Tarrant County\n7         White Male   South     Alabama           Mobile County\n8         White Male   South Mississippi          Jackson County\n9         White Male   South     Florida       Miami-Dade County\n10        Black Male   South   Louisiana East Baton Rouge Parish\n   Foreign.National Execution.Method Execution.Volunteer\n1                no     Firing Squad                 yes\n2                no    Electrocution                  no\n3                no              Gas                 yes\n4                no    Electrocution                 yes\n5                no    Electrocution                 yes\n6                no Lethal Injection                  no\n7                no    Electrocution                  no\n8                no              Gas                  no\n9                no    Electrocution                  no\n10               no    Electrocution                  no\n   Number.of.Victims Number.of.White.Male.Victims\n1                  1                            1\n2                  1                            1\n3                  1                            1\n4                  4                            2\n5                  1                            1\n6                  1                            1\n7                  1                            1\n8                  1                            0\n9                  1                            1\n10                 1                            0\n   Number.of.Black.Male.Victims Number.of.Latino.Male.Victims\n1                             0                             0\n2                             0                             0\n3                             0                             0\n4                             0                             0\n5                             0                             0\n6                             0                             0\n7                             0                             0\n8                             0                             0\n9                             0                             0\n10                            1                             0\n   Number.of.Asian.Male.Victims\n1                             0\n2                             0\n3                             0\n4                             0\n5                             0\n6                             0\n7                             0\n8                             0\n9                             0\n10                            0\n   Number.of.Native.American.Male.Victims\n1                                       0\n2                                       0\n3                                       0\n4                                       0\n5                                       0\n6                                       0\n7                                       0\n8                                       0\n9                                       0\n10                                      0\n   Number.of.Other.Race.Male.Victims Number.of.White.Female.Victims\n1                                  0                              0\n2                                  0                              0\n3                                  0                              0\n4                                  0                              2\n5                                  0                              0\n6                                  0                              0\n7                                  0                              0\n8                                  0                              1\n9                                  0                              0\n10                                 0                              0\n   Number.of.Black.Female.Victims Number.of.Latino.Female.Victims\n1                               0                               0\n2                               0                               0\n3                               0                               0\n4                               0                               0\n5                               0                               0\n6                               0                               0\n7                               0                               0\n8                               0                               0\n9                               0                               0\n10                              0                               0\n   Number.of.Asian.Female.Victims\n1                               0\n2                               0\n3                               0\n4                               0\n5                               0\n6                               0\n7                               0\n8                               0\n9                               0\n10                              0\n   Number.of.Native.American.Female.Victims\n1                                         0\n2                                         0\n3                                         0\n4                                         0\n5                                         0\n6                                         0\n7                                         0\n8                                         0\n9                                         0\n10                                        0\n   Number.of.Other.Race.Female.Victims\n1                                    0\n2                                    0\n3                                    0\n4                                    0\n5                                    0\n6                                    0\n7                                    0\n8                                    0\n9                                    0\n10                                   0\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:37-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-20-using-a-function-to-read-active-duty-marital-data/",
    "title": "Using A Function to Read Active Duty Marital Data",
    "description": "This is the example code to use a new custom function to read in multiple sheets of data about Active Duty members of the military and marital status.",
    "author": [
      {
        "name": "Sean Conway and Meredith Rolfe",
        "url": {}
      }
    ],
    "date": "2021-08-20",
    "categories": [
      "example code",
      "cleaning data",
      "programming functions"
    ],
    "contents": "\nThis is a quick piece of example code to read in the active duty marital status tables from Excel. This example code builds on the earlier examples of reading in Excel tables by creating a function and applying it to multiple sheets.\nCleaning a single sheet\nWe will start off by cleaning a single sheet from the workbook, trying to create a set of generic functions that can be used to iterate through all sheets in the workbook to read them one after the other. To read the single sheet, follow the same process we followed in *Reading in Tabular Data.\"\nIdentify grouping variables and values to extract from the table\nLets first look at an example sheet from the workbook.\nTotal DOD Active Duty Marital SheetWe can see a few things from this example sheet. First, we will need to skip 8 or 9 rows - the data first appears in row 10. Second, the tabular cells represent count values that capture the number of employees falling into subcategories created by 6 distinct grouping values: 1) Pay Grade Type: Enlisted/Officer/Warrent Officer 2) Pay Grade Level: 1-10 (fewer for non-Enlisted) 3) Marital status: Married/Single 4) Parent: Kids/noKids (Single only) 5) Spouse affiliation: Civilian/Military (Married only) 6) Gender: Male/Female\nOur goal is to recover cases that have these 6 (or really 5, if we collapse parent and spouse variables as we don’t have complete information) grouping variables to identify the case and the single value (count of active duty employees who fall into each of the resulting subcategories.)\nLooking back at the original excel sheet, we can see that we will need to not just skip the top rows, we will also need to delete several columns, and also rename variables in order to make it easy to separate out the three pieces of information contained in the column names. First, I create a vector with column names (to make it easier to reuse later in the functional programming) then I read in the data and inspect it to see if the columns worked as intended.\n\n\nmarital <-c(\"d\", \"payGrade_payLevel\",\n            \"single_nokids_male\", \"single_nokids_female\", \"d\",\n            \"single_kids_male\", \"single_kids_female\", \"d\",\n            \"married_military_male\", \"married_military_female\", \"d\",\n            \"married_civilian_male\", \"married_civilian_female\", \"d\",\n            rep(\"d\", 3))\nread_excel(\"../../_data/ActiveDuty_MaritalStatus.xls\", \n           skip=8,\n           col_names = marital\n           )\n\n\n# A tibble: 31 × 17\n   d...1 payGrade_payLevel single_nokids_male single_nokids_fem… d...5\n   <chr> <chr>             <chr>              <chr>              <chr>\n 1 <NA>  Pay Grade         Male               Female             Total\n 2 <NA>  E-1               31229              5717               36946\n 3 <NA>  E-2               53094              8388               61482\n 4 <NA>  E-3               131091             21019              1521…\n 5 <NA>  E-4               112710             16381              1290…\n 6 <NA>  E-5               57989              11021              69010\n 7 <NA>  E-6               19125              4654               23779\n 8 <NA>  E-7               5446               1913               7359 \n 9 <NA>  E-8               1009               438                1447 \n10 <NA>  E-9               381                202                583  \n# … with 21 more rows, and 12 more variables: single_kids_male <chr>,\n#   single_kids_female <chr>, d...8 <chr>,\n#   married_military_male <chr>, married_military_female <chr>,\n#   d...11 <chr>, married_civilian_male <chr>,\n#   married_civilian_female <chr>, d...14 <chr>, d...15 <chr>,\n#   d...16 <chr>, d...17 <chr>\n\nI can see that the variable names worked well, so this time I will read in the data and omit the original header row, and also filter out the various “TOTAL” rows that we don’t need to keep.\n\n\nmilitary<-read_excel(\"../../_data/ActiveDuty_MaritalStatus.xls\", \n           skip=9,\n           col_names = marital\n           )%>%\n  select(!starts_with(\"d\"))%>%\n  filter(str_detect(payGrade_payLevel, \"TOTAL\", negate=TRUE))\nmilitary\n\n\n# A tibble: 24 × 9\n   payGrade_payLev… single_nokids_m… single_nokids_f… single_kids_male\n   <chr>                       <dbl>            <dbl>            <dbl>\n 1 E-1                         31229             5717              563\n 2 E-2                         53094             8388             1457\n 3 E-3                        131091            21019             4264\n 4 E-4                        112710            16381             9491\n 5 E-5                         57989            11021            10937\n 6 E-6                         19125             4654            10369\n 7 E-7                          5446             1913             6530\n 8 E-8                          1009              438             1786\n 9 E-9                           381              202              579\n10 O-1                         13495             3081              402\n# … with 14 more rows, and 5 more variables:\n#   single_kids_female <dbl>, married_military_male <dbl>,\n#   married_military_female <dbl>, married_civilian_male <dbl>,\n#   married_civilian_female <dbl>\n\nIt looks like this worked well! Now we just need to pivot_longer with 3 columns (similar to what we did in the Tabular Data example). Then we will separate out the information in the payGrade_payLevel variable and do a quick mutate to make paygrade easier to remember.\n\n\nmilitary_long <-military %>%\n  pivot_longer(cols = -1,\n               names_to = c(\"Marital\", \"Other\", \"Gender\"),\n               names_sep = \"_\",\n               values_to = \"count\")%>%\n  separate(payGrade_payLevel, \n           into = c(\"payGrade\", \"payLevel\"),\n           sep=\"-\")%>%\n  mutate(payGrade = case_when(\n    payGrade == \"E\" ~ \"Enlisted\",\n    payGrade == \"O\" ~ \"Officer\",\n    payGrade == \"W\" ~ \"Warrant Officer\"\n  ))\nmilitary_long\n\n\n# A tibble: 192 × 6\n   payGrade payLevel Marital Other    Gender count\n   <chr>    <chr>    <chr>   <chr>    <chr>  <dbl>\n 1 Enlisted 1        single  nokids   male   31229\n 2 Enlisted 1        single  nokids   female  5717\n 3 Enlisted 1        single  kids     male     563\n 4 Enlisted 1        single  kids     female   122\n 5 Enlisted 1        married military male     139\n 6 Enlisted 1        married military female   141\n 7 Enlisted 1        married civilian male    5060\n 8 Enlisted 1        married civilian female   719\n 9 Enlisted 2        single  nokids   male   53094\n10 Enlisted 2        single  nokids   female  8388\n# … with 182 more rows\n\nThis all looks like it works well. So now we will go on to creating a function with the steps, then applying it to multiple sheets.\nCreate a new function\nWe will call our new function read_military, and we will basically use the exact same commands as above. The big difference is that we will have a placeholder name (or argument) for the data sheet that will be passed to the new function.\nNext we have to use read_excel(), but we have to specify a number of arguments. We first specify the path (note - file_path is a variable that I created ahead of time to be specific to my computer. Yours will be different). Next, we specify sheet, the number of the sheet we wish to read in (we can also specify the sheet name). We also need to manually specify col_names from our marital vector that we created above.\nmutate() creates a new column called branch, which comes from our sheet name. select(!starts_with(\"d\")) removes all columns that start with \"d\". We also filter out the word “Total” from payGrade_payLevel. pivot_longer()\n\n\nread_military<-function(sheet_name){\n  read_excel(\"../../_data/ActiveDuty_MaritalStatus.xls\", \n             sheet = sheet_name,\n             skip=9,\n             col_names = marital\n             )%>%\n  mutate(\"branch\"=sheet_name) %>%\n  select(!starts_with(\"d\"))%>%\n  filter(str_detect(payGrade_payLevel, \"TOTAL\", negate=TRUE))%>%\n  pivot_longer(cols = contains(c(\"male\", \"female\")),\n               names_to = c(\"Marital\", \"Other\", \"Gender\"),\n               names_sep = \"_\",\n               values_to = \"count\")%>%\n  separate(payGrade_payLevel, \n           into = c(\"payGrade\", \"payLevel\"),\n           sep=\"-\")%>%\n  mutate(payGrade = case_when(\n    payGrade == \"E\" ~ \"Enlisted\",\n    payGrade == \"O\" ~ \"Officer\",\n    payGrade == \"W\" ~ \"Warrant Officer\"\n  ))\n}\n\n\n\nWe now have a function that is customized to read in the mmilitary active duty marital status sheets. We just need to use purrr - a package that is part of tidyverse but which may need to be installed and loaded on its own - to iterate through the list of sheets in the workbook.\n\n\nexcel_sheets(\"../../_data/ActiveDuty_MaritalStatus.xls\")\n\n\n[1] \"TotalDoD\"    \"AirForce\"    \"MarineCorps\" \"Navy\"       \n[5] \"Army\"       \n\nmap_dfr(\n  excel_sheets(\"../../_data/ActiveDuty_MaritalStatus.xls\")[2:5],\n  read_military)\n\n\n# A tibble: 720 × 7\n   payGrade payLevel branch   Marital Other    Gender count\n   <chr>    <chr>    <chr>    <chr>   <chr>    <chr>  <dbl>\n 1 Enlisted 1        AirForce single  nokids   male    7721\n 2 Enlisted 1        AirForce single  nokids   female  1550\n 3 Enlisted 1        AirForce single  kids     male      27\n 4 Enlisted 1        AirForce single  kids     female     5\n 5 Enlisted 1        AirForce married military male      49\n 6 Enlisted 1        AirForce married military female    27\n 7 Enlisted 1        AirForce married civilian male    1064\n 8 Enlisted 1        AirForce married civilian female   178\n 9 Enlisted 2        AirForce single  nokids   male    4380\n10 Enlisted 2        AirForce single  nokids   female  1010\n# … with 710 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-20-wrangling-us-homicide-data/",
    "title": "Wrangling U.S. homicide data",
    "description": "Importing and wrangling U.S. homicide data for Homework 3.",
    "author": [
      {
        "name": "Leah Dion",
        "url": {}
      }
    ],
    "date": "2021-08-20",
    "categories": [
      ".homework 3 .wrangling"
    ],
    "contents": "\nLoading packages necessary for wrangling:\n\n\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\n\nImporting the data and assigning it to variable homi:\nyou will need to fill in the direct URL to the dataset you wish to use, similar to what I did with the executions data.\nIf you are using Kaggle data, you may need to use the API/R Package to access it directly. https://github.com/mkearney/kaggler\nIf that doesn’t work, there is another workaround for accessing datasets from Kaggle or other URL sources: https://stackoverflow.com/questions/55272909/read-dataset-from-kaggle\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:41-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-railroad-county-tracy/",
    "title": "RAILROAD-COUNTY-TRACY",
    "description": "Homework assignment 1, 2 and 3. Loading data into an R Markdown file, Railroad Employment Data.",
    "author": [
      {
        "name": "Erin-Tracy",
        "url": {}
      }
    ],
    "date": "2021-08-19",
    "categories": [],
    "contents": "\nHOMEWORK 2\nTotal Railroad Employment by State and County 2012 breaks down US railroad employment numbers in 2012 by state and county. This dataset explores railroads.\nIt has 120 2930 observations and 63 variables.\nThe three variables are state, county and total employees. I chose to specifically study counties with a very large number of railroad employees. Separately, I also looked into railroad employee numbers just in the New England states.\nSource\nThe dataset is sourced from (https://catalog.data.gov/dataset/total-railroad-employment-by-state-and-county-2012/resource/5a0b2831-23b9-4ce9-82e9-87a7d8f2c5d8)\nFor now, Echo is TRUE, for final version change to FALSE\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nknitr::opts_chunk$set(fig.width = 5, fig.asp = 1/3)\n\n\n\nHOMEWORK 1 Here I am reading in my CSV file.\n\n\ndata<-read.csv(\"../../_data/railroad_2012_clean_county.csv\")\n\n#Head\nhead(data)\n\n\n  state               county total_employees\n1    AE                  APO               2\n2    AK            ANCHORAGE               7\n3    AK FAIRBANKS NORTH STAR               2\n4    AK               JUNEAU               3\n5    AK    MATANUSKA-SUSITNA               2\n6    AK                SITKA               1\n\n#Tail\ntail(data)\n\n\n     state     county total_employees\n2925    WY   SHERIDAN             252\n2926    WY   SUBLETTE               3\n2927    WY SWEETWATER             196\n2928    WY      UINTA              49\n2929    WY   WASHAKIE              10\n2930    WY     WESTON              37\n\n#Dimensions\ndim(data)\n\n\n[1] 2930    3\n\n#Column Names\ncolnames(data)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\nHOMEWORK 3\nExperimenting with Data Transformation\nInitially I didn’t understand this data. I thought that running the count code by county would return to me the number of railroads in each county. Then I reread the description about what this dataset is and realized it’s not about number of railroads, just number of railroad employees and their geographic location. The code count(data,county) is actually particularly useless because all it does is return the number of times that a county name repeats itself across the country. For example, 12 different states have an “Adams County” that has Railroad employees.\nI’m keeping some of the less useful code here for practice, hopefully with an accurate description of what it actually is. I did not run count(data,county) since it takes up a lot of space.\n\n\n#count(data,county)\n\n#Count of Counties by State that have Railroad Employees\ncount(data,state)\n\n\n   state   n\n1     AE   1\n2     AK   6\n3     AL  67\n4     AP   1\n5     AR  72\n6     AZ  15\n7     CA  55\n8     CO  57\n9     CT   8\n10    DC   1\n11    DE   3\n12    FL  67\n13    GA 152\n14    HI   3\n15    IA  99\n16    ID  36\n17    IL 103\n18    IN  92\n19    KS  95\n20    KY 119\n21    LA  63\n22    MA  12\n23    MD  24\n24    ME  16\n25    MI  78\n26    MN  86\n27    MO 115\n28    MS  78\n29    MT  53\n30    NC  94\n31    ND  49\n32    NE  89\n33    NH  10\n34    NJ  21\n35    NM  29\n36    NV  12\n37    NY  61\n38    OH  88\n39    OK  73\n40    OR  33\n41    PA  65\n42    RI   5\n43    SC  46\n44    SD  52\n45    TN  91\n46    TX 221\n47    UT  25\n48    VA  92\n49    VT  14\n50    WA  39\n51    WI  69\n52    WV  53\n53    WY  22\n\n#Among Counties with Railroad Employees, what is the average number of employees in each county \nsummarise(data,avg=mean(total_employees))\n\n\n       avg\n1 87.17816\n\n#Among Counties with Railroad Employees, what is the average number of employees in each county that has railroad employees, by state\ndata %>%\n  group_by(state) %>%\n  summarise(avg=mean(total_employees))\n\n\n# A tibble: 53 × 2\n   state   avg\n   <chr> <dbl>\n 1 AE      2  \n 2 AK     17.2\n 3 AL     63.5\n 4 AP      1  \n 5 AR     53.8\n 6 AZ    210. \n 7 CA    239. \n 8 CO     64.0\n 9 CT    324  \n10 DC    279  \n# … with 43 more rows\n\nExperimenting with prop.table From this data I learned that just under 2% of railroad employees in the US (and the included Canadian county) are in Colorado.\n\n\n#Distribution of Railroad Employees across the US (%)\ndata%>%\n  select(state)%>%\n  table()%>%\n  prop.table()*100\n\n\n.\n        AE         AK         AL         AP         AR         AZ \n0.03412969 0.20477816 2.28668942 0.03412969 2.45733788 0.51194539 \n        CA         CO         CT         DC         DE         FL \n1.87713311 1.94539249 0.27303754 0.03412969 0.10238908 2.28668942 \n        GA         HI         IA         ID         IL         IN \n5.18771331 0.10238908 3.37883959 1.22866894 3.51535836 3.13993174 \n        KS         KY         LA         MA         MD         ME \n3.24232082 4.06143345 2.15017065 0.40955631 0.81911263 0.54607509 \n        MI         MN         MO         MS         MT         NC \n2.66211604 2.93515358 3.92491468 2.66211604 1.80887372 3.20819113 \n        ND         NE         NH         NJ         NM         NV \n1.67235495 3.03754266 0.34129693 0.71672355 0.98976109 0.40955631 \n        NY         OH         OK         OR         PA         RI \n2.08191126 3.00341297 2.49146758 1.12627986 2.21843003 0.17064846 \n        SC         SD         TN         TX         UT         VA \n1.56996587 1.77474403 3.10580205 7.54266212 0.85324232 3.13993174 \n        VT         WA         WI         WV         WY \n0.47781570 1.33105802 2.35494881 1.80887372 0.75085324 \n\nExperimenting with Filter and Arrange I created a subset of data that includes counties that have 1000 or more railroad employees. I named it large_railroadcounties. There are 27 counties with 1000 or more railroad employees.\n\n\n#Filter out counties that have 1000 or more railroad employees\nfilter(data, total_employees>=1000)\n\n\n   state           county total_employees\n1     CA      LOS ANGELES            2545\n2     CA        RIVERSIDE            1567\n3     CA   SAN BERNARDINO            2888\n4     CT        NEW HAVEN            1561\n5     DE       NEW CASTLE            1275\n6     FL            DUVAL            3073\n7     IL             COOK            8207\n8     IL             WILL            1784\n9     IN             LAKE            1999\n10    KS          JOHNSON            1286\n11    MO          JACKSON            2055\n12    NE        BOX BUTTE            1168\n13    NE          DOUGLAS            3797\n14    NE        LANCASTER            1619\n15    NE          LINCOLN            2289\n16    NJ            ESSEX            1097\n17    NY         DUTCHESS            1157\n18    NY           NASSAU            2076\n19    NY           QUEENS            1470\n20    NY          SUFFOLK            3685\n21    NY      WESTCHESTER            1040\n22    PA            BUCKS            1106\n23    PA     PHILADELPHIA            1649\n24    TX           HARRIS            2535\n25    TX          TARRANT            4235\n26    VA INDEPENDENT CITY            3249\n27    WA             KING            1039\n\n#Reassign large railroad counties\nlarge_railroadcounties<- filter(data,total_employees>=1000)\n\n#Head\nhead(large_railroadcounties)\n\n\n  state         county total_employees\n1    CA    LOS ANGELES            2545\n2    CA      RIVERSIDE            1567\n3    CA SAN BERNARDINO            2888\n4    CT      NEW HAVEN            1561\n5    DE     NEW CASTLE            1275\n6    FL          DUVAL            3073\n\n#Count\ncount(large_railroadcounties)\n\n\n   n\n1 27\n\n#Arrange by Total Employees \narrange(large_railroadcounties, desc(total_employees), state, county)\n\n\n   state           county total_employees\n1     IL             COOK            8207\n2     TX          TARRANT            4235\n3     NE          DOUGLAS            3797\n4     NY          SUFFOLK            3685\n5     VA INDEPENDENT CITY            3249\n6     FL            DUVAL            3073\n7     CA   SAN BERNARDINO            2888\n8     CA      LOS ANGELES            2545\n9     TX           HARRIS            2535\n10    NE          LINCOLN            2289\n11    NY           NASSAU            2076\n12    MO          JACKSON            2055\n13    IN             LAKE            1999\n14    IL             WILL            1784\n15    PA     PHILADELPHIA            1649\n16    NE        LANCASTER            1619\n17    CA        RIVERSIDE            1567\n18    CT        NEW HAVEN            1561\n19    NY           QUEENS            1470\n20    KS          JOHNSON            1286\n21    DE       NEW CASTLE            1275\n22    NE        BOX BUTTE            1168\n23    NY         DUTCHESS            1157\n24    PA            BUCKS            1106\n25    NJ            ESSEX            1097\n26    NY      WESTCHESTER            1040\n27    WA             KING            1039\n\nExperimenting with Select These are the counties and states in which there are 1000 or more railroad employees in 1 county.\n\n\nselect(large_railroadcounties,\"state\", \"county\")\n\n\n   state           county\n1     CA      LOS ANGELES\n2     CA        RIVERSIDE\n3     CA   SAN BERNARDINO\n4     CT        NEW HAVEN\n5     DE       NEW CASTLE\n6     FL            DUVAL\n7     IL             COOK\n8     IL             WILL\n9     IN             LAKE\n10    KS          JOHNSON\n11    MO          JACKSON\n12    NE        BOX BUTTE\n13    NE          DOUGLAS\n14    NE        LANCASTER\n15    NE          LINCOLN\n16    NJ            ESSEX\n17    NY         DUTCHESS\n18    NY           NASSAU\n19    NY           QUEENS\n20    NY          SUFFOLK\n21    NY      WESTCHESTER\n22    PA            BUCKS\n23    PA     PHILADELPHIA\n24    TX           HARRIS\n25    TX          TARRANT\n26    VA INDEPENDENT CITY\n27    WA             KING\n\nExperimenting with Filter, Vector, Piping, and Group by\n\n\n#Created subset of data that is just New England states, rename that group \"new_england\"\nnew_england <- filter(data, state %in% c(\"NH\", \"VT\", \"CT\", \"MA\", \"RI\", \"ME\"))\n\n#Among New England Counties with Railroad Employees, what is the average number of employees in each county \nsummarise(new_england, avg=mean(total_employees))\n\n\n       avg\n1 119.4462\n\n#Count of New England Counties by State that have Railroad Employees\ncount(new_england,state)\n\n\n  state  n\n1    CT  8\n2    MA 12\n3    ME 16\n4    NH 10\n5    RI  5\n6    VT 14\n\n#Among New England Counties with Railroad Employees, what is the average number of employees in each county that has railroad employees, by state\nnew_england %>%\n  group_by(state) %>%\n  summarise(avg=mean(total_employees))\n\n\n# A tibble: 6 × 2\n  state   avg\n  <chr> <dbl>\n1 CT    324  \n2 MA    282. \n3 ME     40.9\n4 NH     39.3\n5 RI     97.4\n6 VT     18.5\n\nExperimenting with Advanced Functions Working with Renaming and Pivot_longer. I want to get more familiar with pivot longer, but I don’t think there are enough variables in this dataset to really experiment with it.\n\n\n#rename\ndata<-rename(data,employees = total_employees)\ncolnames(data)\n\n\n[1] \"state\"     \"county\"    \"employees\"\n\n#new_england<-rename(new_england,employees = total_employees)\ncolnames(new_england)\n\n\n[1] \"state\"           \"county\"          \"total_employees\"\n\n#relocate()\n\n#across()/c_across()\n\n#pivot_longer()/pivot_wider()\n#results are lengthy and not useful\n#data%>%\n  #pivot_longer(cols=employees,\n               #names_to=\"Type\",\n               #values_to=\"numberemployees\")\n            \n#purrr::map()\n\n#lapply()\n\n\n\nExperimenting with Advanced Functions Cont.\n\n\n#case_when()\n#Assign the words Large, medium and small to specific numeric values for number of employees\ndata<-data%>%\n  mutate(Railroad_size = case_when(\n         employees >= 1000 ~ \"Large\",\n         employees >= 500 & employees < 1000 ~ \"Medium\",\n         employees < 500 ~ \"Small\"))\n\n#See how many counties from full dataset have a small, medium and large amount of railroad employees\ntable(select(data, Railroad_size))\n\n\n\n Large Medium  Small \n    27     65   2838 \n\n#See how many counties in New England have a small, medium or large amount of railroad employees\nnew_england<-new_england%>%\n  mutate(Railroad_size = case_when(\n         total_employees >= 1000 ~ \"Large\",\n         total_employees >= 500 & total_employees < 1000 ~ \"Medium\",\n         total_employees < 500 ~ \"Small\"))\n\ntable(select(new_england, Railroad_size))\n\n\n\n Large Medium  Small \n     1      2     62 \n\n#Use crosstabs to which new england state have counties with a small, medium and large amount of railroad employees\nxtabs(~state+ Railroad_size,new_england)\n\n\n     Railroad_size\nstate Large Medium Small\n   CT     1      0     7\n   MA     0      2    10\n   ME     0      0    16\n   NH     0      0    10\n   RI     0      0     5\n   VT     0      0    14\n\nExperimenting with ggplot, boxplot, labels Connecticut has an outlier. Connecticut has 1 county with an especially large number of railroad employees.\n\n\n#Boxplot for New England State Counties\nggplot(new_england,aes(state,total_employees))+geom_boxplot()+\nlabs(title = \"Railroad Employee County Counts by State, NE\", y = \"Total Employees\", x = \"State\") \n\n\n\n\nSame data, just shown differently\n\n\n#Geompoints for New England State Counties\nggplot(new_england,aes(state,total_employees))+\n  geom_point()+\n  geom_smooth()+\n  labs(title = \"Railroad Employee County Counts by State, NE\", y = \"Total Employees\", x = \"State\") \n\n\n\n\nExperimenting with ggplot and fill\nI would love to use fill, but it doesn’t make sense for this dataset. It doesn’t make sense because my third variable (county) is basically different for every state. This would be much more useful if that variable was something that had valuables that were applicable to all states.\n\n\n#Geomplot for New England States with County filled (Two Ways)\n#ggplot(new_england,aes(state, fill=county))+ geom_bar()+\n  #theme_bw()+\n  #labs(title=\"New England States Railroad Employee Counts by State and County\", y=\"Number of Employees\", x= \"State\")\n\n#ggplot(data=new_england)+\n  #geom_bar(mapping=aes(x=state, fill=county))\n  #theme_bw()+\n  #labs(title=\"New England States Railroad Employee Counts by State and County\", y=\"Number of Employees\", x= \"State\")\n\n\n\nExperimenting with geompoint, with different dataset\nIllinois has 1 county that has over 8000 railroad employees\n\n\nggplot(data=large_railroadcounties)+\n  geom_point(mapping=aes(x=state, y=total_employees))+\n  labs(title = \"States with counties with 1000+ Railroad Employees\", y = \"Total Employees\", x = \"State\") \n\n\n\n\n\nSaving this code shared by Larri\n#blackturnout <- blackturnout %>%\n  #mutate(candidateRename = recode(candidate, `1` = \"co-ethnic\", `0` = \"not co-ethnic\"))\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\n\nLearn more about using Distill at <https://rstudio.github.io/distill>.\n\n\n\n\n```{.r .distill-force-highlighting-css}\n\n\n",
    "preview": "posts/2021-08-17-railroad-county-tracy/railroad-county-tracy_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-08-24T08:21:32-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 320
  },
  {
    "path": "posts/2021-08-19-carbon-emissions/",
    "title": "Final Project",
    "description": "Baby steps",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": {}
      }
    ],
    "date": "2021-08-19",
    "categories": [],
    "contents": "\n#Code Setup\n#Data Description\n\n\n\n#Data Questions\n\n1.a Do you approve or disapprove of the way Joe Biden is handling…the response to the coronavirus(COVID-19)? 1.b Do you approve or disapprove of the way Joe Biden is handling…immigration and the situation at the United States-Mexico border? 1.c Do you approve or disapprove of the way Joe Biden is handling…the economic recovery? 1.d Do you approve or disapprove of the way Joe Biden is handling…the withdrawal of United States troops from Afghanistan? 1.e Do you approve or disapprove of the way Joe Biden is handling…gun violence? 1.f Do you approve or disapprove of the way Joe Biden is handling…crime? 2 How concerned are you that you or someone you know will be infected with the coronavirus?… 3 Have you personally received at least one dose of a coronavirus vaccine, or not? 4 How would you rate the job Joe Biden is doing keeping his campaign promises?…. 5 Thinking about the next 12 months, would you say you feel optimistic or pessimistic about the way things are going in this country?\n\n#Exploratory Analysis\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:13-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-19-school-district-data-tracy/",
    "title": "SCHOOL-DISTRICT-DATA-TRACY",
    "description": "Data Analysis of Massachusetts Public Schools' Class Size by Gender and Selected Populations for DACSS601 Final Project, HW4 and HW5",
    "author": [],
    "date": "2021-08-19",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(\"tidyverse\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\n\nknitr::opts_chunk$set(fig.width = 5, fig.asp = 1/3)\n\n\n\nINTRODUCTION\nThe Class Size by Gender and Selected Populations data represents the number of classes and average class size for each subject by gender, Limited English Proficient and Low Income for public school districts in Massachusetts.\nAccording to this data, Massachusetts Public School Districts have on average 1142 total number of classes with an average class size of 17.7. The average Massachusetts Public School District has 2382 students with 48.8% identifying as Female and 50.9% identifying as Male. On average, 6.66% of students are English Language Learners (ELLs), 18.47% of students have disabilities and 30.53% are Economically Disadvantaged.\nThe purpose of this analysis is to compare the data from school districts with a high number of economically disadvantaged students to districts with a low number of economically disadvantaged students. My prediction is that schools with larger percentages of economically disadvantaged students will also have larger populations of students with disabilities and of English Language Learners, as well as larger class sizes. I predict that percentages of male and female students will be the same regardless of the economically disadvantaged variable.\nI think this is important data to study because if my predictions are correct, further research should be done to look into the breakdown of state funding for districts with greater numbers of economically disadvantaged students. I am assuming that districts with more economically disadvantaged students do not receive the same amount of local funding per student as districts with fewer economically disadvantaged students, and so they would rely on the state to even the playing field. And if school districts with greater numbers of economically disadvantaged students do in fact also have more ELLs and students with disabilities, those districts actually need more funding per student, given that students who are ELLs and students with disabilities need more support services than other students.\nIf my predictions are correct that class sizes are larger among districts with more economically disadvantaged students, again all signs point to more research on funding. Class sizes can only decrease with more staff and more space, both of which require additional funding. For more information on the benefits of smaller class size and student learning, please refer to the multiple sources available at the website below.\nhttps://classsizematters.org/research-and-links/\nI am the daughter of a public school teacher, the mother of a public school Pre-Kindergartener and an employee of the state’s largest teacher’s union. I am passionately pro-public education. I think we all need to constantly be checking to make sure that public school students across the state are all receiving a quality education. Comparing the variables of this data set is one way to see if students are having different experiences, and if those differences are at all related to economic situation.\nDATA\nThe 9 variables are District, Total Number of Classes (Classes), Average Class Size (avgclasssize), Number of Students (students), Percentage of Students who identify as Male (malepercent), Percentage of Students who identify as Female (femalepercent), Percentage of students that are English Language Learners(ellpercent), Percentage of Students with Disabilities (studentswdisabilitiespercent) and Percentage of students that are Economically Disadvantaged (econdisadvantagedpercent). Number of students includes Pre-Kindergarten through 12th grade students. All “Percentage of” student data is out of the entire school district population. Additional definitions are included below.\nThere are 403 observations (school districts). Please note that only public school districts are included. This includes some public charter schools, but does not include private charter schools or any private schools in Massachusetts. Some observations include missing data.\nMore information is also available at the original source: https://profiles.doe.mass.edu/statereport/classsizebygenderpopulation.aspx\nData sourced from SIMS (Student Information Management Systems), SCS (Student Course Schedule) and EPIMS (Education Personnel Information Management System)\nDEFINITIONS\nTotal # of Classes: Number of classes is based on classes that could be linked between SIMS, EPIMS, and SCS by School, Course, Section, and Term. The class must have a class count greater than 1 (one) and have students in the class who have a SCS enrollment status of enrolled, completed, or incomplete.\nAverage Class Size: Average Class Size is calculated by dividing the total number of students in classes by the total number of classes. Students taking multiple classes will be included in multiple class size averages.\nNumber of Students: This figure is de-duplicated count of students at the subject, district, and school levels.\nEnglish Learners: Indicates the percent of enrollment who are English learners, defined as \"a student whose first language is a language other than English who is unable to perform ordinary classroom work in English\nEconomically Disadvantaged: Calculated based on a student’s participation in one or more of the following state-administered programs: the Supplemental Nutrition Assistance Program (SNAP); the Transitional Assistance for Families with Dependent Children (TAFDC); the Department of Children and Families’ (DCF) foster care program; and MassHealth (Medicaid).\nThe Massachusetts Department of Elementary and Secondary Education (DESE) is the state education agency for Massachusetts. One of DESE’s primary purposes is to collect data to inform state and local decisions. MA DESE’s website includes hundreds of different data sets.\nHOMEWORK 4\nWhile reading in this data, I did remove the School District ID column, which may be useful in future research. (Perhaps if someone was interested in combining various DESE datasets into one.)\nI also had some observations that included “0” as a value for all variables.That seemed to be meaningless since a district could not have 0 students, 0 classes, etc. I was concerned that keeping that data in the dataset would incorrectly skew my analysis. Including a “0” in a small dataset, when calculating average would result in an incorrect low average. Thanks to the #r-help slack page, I was able to receive advice as to what code would help me correct this issue.\n\n\ndata<-read_csv(\"ClassSizebyGenPopulation.csv\",skip=2,\n               col_names= c(\"District\", \"delete\", \"classes\", \"avgclasssize\", \"students\",\"femalepercent\",\"malepercent\",\"ellpercent\",\"studentswdisabilitiespercent\",\"econdisadvantagedpercent\"))%>%\n  select(!contains(\"delete\"))\n\ndata.missing <- data%>%\n  mutate(allna = rowSums(select(., classes:econdisadvantagedpercent)),\n         across(classes:econdisadvantagedpercent, ~case_when(\n           . == 0 ~ NA_real_,\n           . > 0 ~ .)))%>%\n  filter(allna ==0)%>%\n  select(-allna)\n\nhead(data.missing)\n\n\n# A tibble: 1 × 9\n  District classes avgclasssize students femalepercent malepercent\n  <chr>      <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1 Gosnold       NA           NA       NA            NA          NA\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\ndata.clean <- data%>%\n  mutate(allna = rowSums(select(., classes:econdisadvantagedpercent)),\n         across(classes:econdisadvantagedpercent, ~case_when(\n           . == 0 ~ NA_real_,\n           . > 0 ~ .)))%>%\n  filter(!allna ==0)%>%\n  select(-allna)\n\nhead(data.clean)\n\n\n# A tibble: 6 × 9\n  District     classes avgclasssize students femalepercent malepercent\n  <chr>          <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1 Abby Kelley…     711         19       1424          54          46  \n2 Abington        1034         22.4     2194          48.9        51.1\n3 Academy Of …     185         19.1      539          49.5        50.5\n4 Acton-Boxbo…    2190         20.2     5463          48.2        51.8\n5 Acushnet         546         20.2     1016          47          53.1\n6 Advanced Ma…     510         18.1      964          47.6        52.4\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\nHere I ran some very basic code to help be understand this data.\n\n\nsummarise_all(data.clean, mean)\n\n\n# A tibble: 1 × 9\n  District classes avgclasssize students femalepercent malepercent\n     <dbl>   <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1       NA   1144.         17.8    2388.          48.9        51.1\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\ndim(data.clean)\n\n\n[1] 402   9\n\ncolnames(data.clean)\n\n\n[1] \"District\"                     \"classes\"                     \n[3] \"avgclasssize\"                 \"students\"                    \n[5] \"femalepercent\"                \"malepercent\"                 \n[7] \"ellpercent\"                   \"studentswdisabilitiespercent\"\n[9] \"econdisadvantagedpercent\"    \n\nhead(data.clean)\n\n\n# A tibble: 6 × 9\n  District     classes avgclasssize students femalepercent malepercent\n  <chr>          <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1 Abby Kelley…     711         19       1424          54          46  \n2 Abington        1034         22.4     2194          48.9        51.1\n3 Academy Of …     185         19.1      539          49.5        50.5\n4 Acton-Boxbo…    2190         20.2     5463          48.2        51.8\n5 Acushnet         546         20.2     1016          47          53.1\n6 Advanced Ma…     510         18.1      964          47.6        52.4\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\ntail(data.clean)\n\n\n# A tibble: 6 × 9\n  District    classes avgclasssize students femalepercent malepercent\n  <chr>         <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1 Winchester     2416         19.3     4722          50          50  \n2 Winthrop        698         20.5     1998          51.1        48.9\n3 Woburn         2711         16.8     4544          50.3        49.7\n4 Worcester     12818         15.6    25186          48.6        51.4\n5 Worthington      50         10.6       94          48.9        51.1\n6 Wrentham        422         18.2      967          48.4        51.6\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\nHOMEWORK 5\nI created 3 ggplot graphs to provide preliminary visual graphics to help me answer the specific question, “are class sizes larger among school districts with the greatest number of economically disadvantaged students?” The geomsmooth ggplot graph suggests “yes, slightly”, but the ggplots with geom_points are less decisive. Clearly further analysis is necessary.\nI found the ggplot with geompoint and geomsmooth to provide the most useful visual for this type of question. I repeated the same type of ggplot graph for the question “are there more students with disabilities at schools with the greatest number of economically disadvantaged students?” Again, it looks like the answer is yes, but only slightly.\nThe final ggplot graph in this section addresses the question, “are there more Engligh Language Learners at schools with the greatest number of economically disadvantaged students?” The answer to this question seems to be a more cetain “yes”. The Percentage of ELL students definitely seems to increase as the percentage of economically disadvantaged students increases.\nMy next plan is to mutate the “econdisadvantagedpercent” data so that the variables are named ranges (tbd), for example 0-10%, 11-20%, 21-30%…. and name them “very low”, “low”, “medium”, “high”, and “very higher”. I will then run additional graphs, possibly using facets and fill. I’ve been challenged because all of the values for all of the variables are integers, I think I’ll be able to produce better visuals and see results more clearly with the updates mentioned here.\n\n\nggplot(data = data.clean)+\n  geom_point(mapping = aes(x= econdisadvantagedpercent, y=avgclasssize))\n\n\n\nggplot(data = data.clean)+\n  geom_smooth(mapping = aes(x= econdisadvantagedpercent, y=avgclasssize))\n\n\n\nggplot(data = data.clean) + \n  geom_point(mapping = aes(x = econdisadvantagedpercent, y = avgclasssize)) +\n  geom_smooth(mapping = aes(x = econdisadvantagedpercent, y = avgclasssize))\n\n\n\nggplot(data = data.clean) + \n  geom_point(mapping = aes(x = econdisadvantagedpercent, y = studentswdisabilitiespercent)) +\n  geom_smooth(mapping = aes(x = econdisadvantagedpercent, y = studentswdisabilitiespercent))\n\n\n\nggplot(data = data.clean) + \n  geom_point(mapping = aes(x = econdisadvantagedpercent, y = ellpercent)) +\n  geom_smooth(mapping = aes(x = econdisadvantagedpercent, y = ellpercent))\n\n\n\n\nI created a subset of data for the 40 districts (roughly 10%) with the most economically disadvantaged students (high_econdis_data) and another for the 40 districts (roughly 10%) with the least economically disadvantaged students (low_econdis_data). I ran some quick averages to see some comparisons.\nThe “Low Economically Disadvantaged” district data have an average of 5.70% economically disadvantaged students, 2.22% English Language Learners and 15.1% students with disabilities.\nComparatively, the “High Economically Disadvantaged” district data has an average of 69.5% economically disadvantaged students, 19.7% English Language Learners and 22% students with disabilities.\nThis all seems consistent with the ggplots above.\nThe “Low Economically Disadvantaged” districts have an average class size of 17.6 and 2268 total students in the district. The “High Economically Disadvantaged” districts have an average class size of 19.4 and 3852 total students in the district.\nOne thing I found interesting, and unexpected was regarding gender identification.\nAmong the “High Economically Disadvantaged” districts, 49.6% identify as female and 50.3% identify as male, leaving approximately 0.1% of students as not identifying strictly as male or female. In the “Low Economically Disadvantaged” districts 48.4% identify as female and 49.0% identify as male, leaving approximately 2.6% as not identifying strictly as male or female.\nI initially only included 10 districts in each of these subsets, but it seemed like such a small subset would not be as useful as each subset would be made up primarily of outliers.\nSince I ran the same code (summarize_all(mean)) multiple times, I think maybe it would be more efficient to create my own code, but I’m still trying to figure that out.\n\n\ndata<- arrange(data.clean,desc(`econdisadvantagedpercent`))\n\nhigh_econdis_data<- head(data.clean,40)\nlow_econdis_data<- tail(data.clean,40)\n\nsummarise_all(high_econdis_data, mean)\n\n\n# A tibble: 1 × 9\n  District classes avgclasssize students femalepercent malepercent\n     <dbl>   <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1       NA   1037.         18.7    2202.          48.8        51.1\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\nsummarise_all(low_econdis_data, mean)\n\n\n# A tibble: 1 × 9\n  District classes avgclasssize students femalepercent malepercent\n     <dbl>   <dbl>        <dbl>    <dbl>         <dbl>       <dbl>\n1       NA   1459.         17.5    3004.          48.5        51.5\n# … with 3 more variables: ellpercent <dbl>,\n#   studentswdisabilitiespercent <dbl>,\n#   econdisadvantagedpercent <dbl>\n\nlow_econdis_data%>%\n  select(ellpercent, studentswdisabilitiespercent, econdisadvantagedpercent)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 3\n  ellpercent studentswdisabilitiespercent econdisadvantagedpercent\n       <dbl>                        <dbl>                    <dbl>\n1         NA                         18.2                     28.1\n\nhigh_econdis_data%>%\n  select(ellpercent, studentswdisabilitiespercent, econdisadvantagedpercent)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 3\n  ellpercent studentswdisabilitiespercent econdisadvantagedpercent\n       <dbl>                        <dbl>                    <dbl>\n1       6.74                         17.3                     29.3\n\nlow_econdis_data%>%\n  select(avgclasssize, students)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 2\n  avgclasssize students\n         <dbl>    <dbl>\n1         17.5    3004.\n\nhigh_econdis_data%>%\n  select(avgclasssize, students)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 2\n  avgclasssize students\n         <dbl>    <dbl>\n1         18.7    2202.\n\nlow_econdis_data%>%\n  select(femalepercent, malepercent)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 2\n  femalepercent malepercent\n          <dbl>       <dbl>\n1          48.5        51.5\n\nhigh_econdis_data%>%\n  select(femalepercent, malepercent)%>%\n  summarize_all(mean)\n\n\n# A tibble: 1 × 2\n  femalepercent malepercent\n          <dbl>       <dbl>\n1          48.8        51.1\n\n\n\n\nREFLECTION\n(work in progress) Should not have called data, data should have had shorter variable names\nhttps://www.doe.mass.edu/About.html https://profiles.doe.mass.edu/statereport/classsizebygenderpopulation.aspx https://classsizematters.org/research-and-links/\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-19-school-district-data-tracy/school-district-data-tracy_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-08-24T08:22:20-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 320
  },
  {
    "path": "posts/2021-08-18-ankithw2/",
    "title": "Ankit_HW2",
    "description": "A short description of the IRIS dataset.",
    "author": [
      {
        "name": "Ankit Kumar",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n[5] \"Species\"     \n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-18-ankithw2/ankithw2_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-08-24T08:21:34-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-18-interest-rate-inflation-and-unemployment-rates/",
    "title": "Interest Rate, Inflation, and Unemployment rates",
    "description": "A comparison of interest rates vs GDP change vs inflation vs unemployment rates in the US from 1954 - 2017",
    "author": [
      {
        "name": "Ben Lewis",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\nReading in my data.\nThis set contains monthly Federal Reserve interest rates from 1954-2017. It also contains the quarterly inflation rates, and monthly unemployment rates. I downloaded this data from Kaggle. Raw data can be found here\n\n\nfed_rates <- read_csv(\"../../_data/FedFundsRate.csv\")\n\n\n\nFixing dates\nChanging numeric values in column “Year” to names of the month. I want to simplify the data set by being able to see the names of the month instead of month number.\n\n\nfed_rates <- fed_rates %>%\n  mutate(Month = case_when(\n         Month == 1 ~ \"January\",\n         Month == 2 ~ \"February\",\n         Month == 3 ~ \"March\",\n         Month == 4 ~ \"April\",\n         Month == 5 ~ \"May\",\n         Month == 6 ~ \"June\",\n         Month == 7 ~ \"July\",\n         Month == 8 ~ \"August\", \n         Month == 9 ~ \"September\",\n         Month == 10 ~ \"October\",\n         Month == 11 ~ \"November\",\n         Month == 12 ~ \"December\")\n  )\n\n\n\nRemove variables\nThere are variables in the fed_rates data set that I am not concerned with. These columns are the Fed Funds Target Rate, Fed Funds Upper Target Rate, and Fed Funds Lower Target Rate. Taking my variables from 10 to 7 by removing those columns and reducing noise in the data.\n\n\nfed_rates_new <- fed_rates %>%\n  select(\"Year\" , \"Month\" ,  \"Day\" , \"Effective Federal Funds Rate\" ,  \"Real GDP (Percent Change)\",  \"Unemployment Rate\" ,  \"Inflation Rate\")\n\nfed_rates_new\n\n\n# A tibble: 904 × 7\n    Year Month       Day `Effective Federal Fun… `Real GDP (Percent C…\n   <dbl> <chr>     <dbl>                   <dbl>                 <dbl>\n 1  1954 July          1                    0.8                    4.6\n 2  1954 August        1                    1.22                  NA  \n 3  1954 September     1                    1.06                  NA  \n 4  1954 October       1                    0.85                   8  \n 5  1954 November      1                    0.83                  NA  \n 6  1954 December      1                    1.28                  NA  \n 7  1955 January       1                    1.39                  11.9\n 8  1955 February      1                    1.29                  NA  \n 9  1955 March         1                    1.35                  NA  \n10  1955 April         1                    1.43                   6.7\n# … with 894 more rows, and 2 more variables:\n#   Unemployment Rate <dbl>, Inflation Rate <dbl>\n\nFiltering by Quarter\nto make viewing quarterly GDP growth rate data easier to track. Because we do not tend to see wild swings in monthly data in regards to inflation and interest rate change.\n\n\nfed_rates_quarter <- fed_rates_new %>%\n  filter(`Month` %in% c(\"January\" , \"April\" , \"July\" , \"October\"))\n\nfed_rates_quarter\n\n\n# A tibble: 295 × 7\n    Year Month     Day `Effective Federal Funds Rate` `Real GDP (Perc…\n   <dbl> <chr>   <dbl>                          <dbl>            <dbl>\n 1  1954 July        1                           0.8               4.6\n 2  1954 October     1                           0.85              8  \n 3  1955 January     1                           1.39             11.9\n 4  1955 April       1                           1.43              6.7\n 5  1955 July        1                           1.68              5.5\n 6  1955 October     1                           2.24              2.4\n 7  1956 January     1                           2.45             -1.5\n 8  1956 April       1                           2.62              3.4\n 9  1956 July        1                           2.75             -0.3\n10  1956 October     1                           2.96              6.7\n# … with 285 more rows, and 2 more variables:\n#   Unemployment Rate <dbl>, Inflation Rate <dbl>\n\nReal GDP (Percent Change) over time\nWe see a downward annual trend since 1954 with fairly substantial movement in GDP percent growth quarterly. The highest % change in GDP came in Q2 (April) of 1978 with a 16% increase QoQ.\n\n\nsummarise_GDP <- fed_rates_quarter \nselect(fed_rates_quarter,`Month`, `Real GDP (Percent Change)`)\n\n\n# A tibble: 295 × 2\n   Month   `Real GDP (Percent Change)`\n   <chr>                         <dbl>\n 1 July                            4.6\n 2 October                         8  \n 3 January                        11.9\n 4 April                           6.7\n 5 July                            5.5\n 6 October                         2.4\n 7 January                        -1.5\n 8 April                           3.4\n 9 July                           -0.3\n10 October                         6.7\n# … with 285 more rows\n\n  summarise(fed_rates_quarter , mean.Real_GDP = mean(`Real GDP (Percent Change)` , na.rm = TRUE) , max.Real_GDP = max(`Real GDP (Percent Change)` , na.rm = TRUE) , min.Real_GDP = min(`Real GDP (Percent Change)` , na.rm = TRUE) , sd.Real_GDP = sd(`Real GDP (Percent Change)` , na.rm = TRUE) , IQR.Real_GDP = IQR(`Real GDP (Percent Change)` , na.rm = TRUE))\n\n\n# A tibble: 1 × 5\n  mean.Real_GDP max.Real_GDP min.Real_GDP sd.Real_GDP IQR.Real_GDP\n          <dbl>        <dbl>        <dbl>       <dbl>        <dbl>\n1          3.14         16.5          -10        3.60         3.48\n\n\n\nggplot(fed_rates_quarter, aes(`Year` , `Real GDP (Percent Change)`)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nUnemployment rate over time\n\n\nggplot(fed_rates_quarter, aes(`Year` , `Unemployment Rate`)) + \n  geom_point() + \n  geom_smooth() \n\n\n\n\nInterest Rate over time\n\n\nggplot(fed_rates_quarter, aes(`Year` , `Effective Federal Funds Rate`)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\nInflation Rate over time\n\n\n ggplot(fed_rates_quarter, aes(`Year` , `Inflation Rate`)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-18-interest-rate-inflation-and-unemployment-rates/interest-rate-inflation-and-unemployment-rates_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-08-24T08:21:41-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-18-reading-in-tabular-data/",
    "title": "Reading in Tabular Data",
    "description": "Two examples of reading in Excel Tables with\nthe advanced data sets.",
    "author": [
      {
        "name": "Meredith Rolfe",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [
      "example code",
      "data cleaning"
    ],
    "contents": "\nThis post will take a closer look at some tools that can be used to read in tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail below, but here is an overview. Note that not every step is needed for every file.\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nColumn issues usually addressed during data read-in\nRow issues usually addressed using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer`, etc\nRailroad data\nThe railroad data set is a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination.\n Looking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nread_excel(\"../../_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\n# A tibble: 2,990 × 5\n   STATE     ...2  COUNTY               ...4  TOTAL\n   <chr>     <lgl> <chr>                <lgl> <dbl>\n 1 AE        NA    APO                  NA        2\n 2 AE Total1 NA    <NA>                 NA        2\n 3 AK        NA    ANCHORAGE            NA        7\n 4 AK        NA    FAIRBANKS NORTH STAR NA        2\n 5 AK        NA    JUNEAU               NA        3\n 6 AK        NA    MATANUSKA-SUSITNA    NA        2\n 7 AK        NA    SITKA                NA        1\n 8 AK        NA    SKAGWAY MUNICIPALITY NA       88\n 9 AK Total  NA    <NA>                 NA      103\n10 AL        NA    AUTAUGA              NA      102\n# … with 2,980 more rows\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nread_excel(\"../../_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\n# A tibble: 2,990 × 3\n   State     County               Employees\n   <chr>     <chr>                    <dbl>\n 1 AE        APO                          2\n 2 AE Total1 <NA>                         2\n 3 AK        ANCHORAGE                    7\n 4 AK        FAIRBANKS NORTH STAR         2\n 5 AK        JUNEAU                       3\n 6 AK        MATANUSKA-SUSITNA            2\n 7 AK        SITKA                        1\n 8 AK        SKAGWAY MUNICIPALITY        88\n 9 AK Total  <NA>                       103\n10 AL        AUTAUGA                    102\n# … with 2,980 more rows\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nrailroad<-read_excel(\"../../_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\nrailroad\n\n\n# A tibble: 2,933 × 3\n   State County               Employees\n   <chr> <chr>                    <dbl>\n 1 AE    APO                          2\n 2 AK    ANCHORAGE                    7\n 3 AK    FAIRBANKS NORTH STAR         2\n 4 AK    JUNEAU                       3\n 5 AK    MATANUSKA-SUSITNA            2\n 6 AK    SITKA                        1\n 7 AK    SKAGWAY MUNICIPALITY        88\n 8 AL    AUTAUGA                    102\n 9 AL    BALDWIN                    143\n10 AL    BARBOUR                      1\n# … with 2,923 more rows\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\ntail(railroad, 10)\n\n\n# A tibble: 10 × 3\n   State                                               County Employees\n   <chr>                                               <chr>      <dbl>\n 1 WY                                                  PLATTE       129\n 2 WY                                                  SHERI…       252\n 3 WY                                                  SUBLE…         3\n 4 WY                                                  SWEET…       196\n 5 WY                                                  UINTA         49\n 6 WY                                                  WASHA…        10\n 7 WY                                                  WESTON        37\n 8 CANADA                                              <NA>         662\n 9 1  Military designation.                            <NA>          NA\n10 NOTE:  Excludes 2,896 employees without an address. <NA>          NA\n\n#remove the last two observations\nrailroad <-head(railroad, -2)\n\ntail(railroad, 10)\n\n\n# A tibble: 10 × 3\n   State  County     Employees\n   <chr>  <chr>          <dbl>\n 1 WY     NIOBRARA          51\n 2 WY     PARK              29\n 3 WY     PLATTE           129\n 4 WY     SHERIDAN         252\n 5 WY     SUBLETTE           3\n 6 WY     SWEETWATER       196\n 7 WY     UINTA             49\n 8 WY     WASHAKIE          10\n 9 WY     WESTON            37\n10 CANADA <NA>             662\n\nRegenerating grouped totals\nAnd that is all it takes! The data are now ready for analysis. For example, suppose we wished to recover the information about state totals. This is easy to do using group_by.\n\n\nrailroad%>%\n  group_by(State)%>%\n  summarise(`State Employees` = sum(Employees))\n\n\n# A tibble: 54 × 2\n   State  `State Employees`\n   <chr>              <dbl>\n 1 AE                     2\n 2 AK                   103\n 3 AL                  4257\n 4 AP                     1\n 5 AR                  3871\n 6 AZ                  3153\n 7 CA                 13137\n 8 CANADA               662\n 9 CO                  3650\n10 CT                  2592\n# … with 44 more rows\n\nAustralian Marriage Data\nThis is another government published tabular data source. In 2017, Australia conducted a postal survey to gauge citizens’ opinions towards same sex marriage. The survey questions was straightforward: “Should the law be changed to allow same-sex couples to marry?” Here is a quick image showing the original table format.\n While similar in some respect to the State Railroad data above, the Australian survey data are clearly more complex in several respects. - There are two values (vote count and percentage) in the dataset - The values appear to be redundent (percentage is easy to recover from vote count data) - There may be other redundant information in - Grouped information instead of individual observations where variables appear elsewhere - Redundent vvariables (Total:Response Clear on the left and ResponeClear)\nIdentify desired data structure\nIf we decide to temporarily ignore the proportions data (as suggested above) and the “totals” columns, we can identify four potentially distinct pieces of information in addition to the vote count columns: County, Division, response(yes, no, response unclear, and non-responding) and vote count. Our goal is to create this desirable data set.\nRepeating steps from above\nWe will once again use skip and col_names to read in the data, select to get rid of unneeded columns, and filter to get rid of unneeded rows. We also use the drop_na function to filter unwanted rows.\n\n\nvotes <- read_excel(\"../../_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"Town\", \"Yes\", \"d\", \"No\", rep(\"d\", 6), \"Illegible\", \"d\", \"No Response\", rep(\"d\", 3)))%>%\n  select(!starts_with(\"d\"))%>%\n  drop_na(Town)%>%\n  filter(!str_detect(Town, \"(Total)\"))%>%\n  filter(!str_starts(Town, \"\\\\(\"))\n\nvotes\n\n\n# A tibble: 160 × 5\n   Town                        Yes    No Illegible `No Response`\n   <chr>                     <dbl> <dbl>     <dbl>         <dbl>\n 1 New South Wales Divisions    NA    NA        NA            NA\n 2 Banks                     37736 46343       247         20928\n 3 Barton                    37153 47984       226         24008\n 4 Bennelong                 42943 43215       244         19973\n 5 Berowra                   48471 40369       212         16038\n 6 Blaxland                  20406 57926       220         25883\n 7 Bradfield                 53681 34927       202         17261\n 8 Calare                    54091 35779       285         25342\n 9 Chifley                   32871 46702       263         28180\n10 Cook                      47505 38804       229         18713\n# … with 150 more rows\n\nAt this point, you can see we are REALLY close. We have yes and no variable plus illegible and no response. That said, tthe current step is more complicated. Each observation (county) needs a variable for administrative “division”, but this is displayed at the top of each block.\nThe following code uses case_when to make a new “Divisions” variables with an entry (e.g., New South Wales Division) where there is a Division name in the town column, and otherwise create just an empty space.\nAt that point, the following loop (with seq_along) can be used to fill in empty spaces with the most recent Divisions name, and then filter out rows with only the title information.\n\n\nvotes<- votes%>%\n  mutate(Divisions = case_when(\n    str_ends(Town, \"Divisions\") ~ Town,\n    TRUE ~ NA_character_\n  ))\n\nfor(i in seq_along(votes$Divisions)){\n  votes$Divisions[i]<-ifelse(is.na(votes$Divisions[i]),votes$Divisions[i-1], votes$Divisions[i])\n}\n\nvotes<- filter(votes,!str_detect(Town, \"Divisions|Australia\"))\nvotes\n\n\n# A tibble: 150 × 6\n   Town        Yes    No Illegible `No Response` Divisions            \n   <chr>     <dbl> <dbl>     <dbl>         <dbl> <chr>                \n 1 Banks     37736 46343       247         20928 New South Wales Divi…\n 2 Barton    37153 47984       226         24008 New South Wales Divi…\n 3 Bennelong 42943 43215       244         19973 New South Wales Divi…\n 4 Berowra   48471 40369       212         16038 New South Wales Divi…\n 5 Blaxland  20406 57926       220         25883 New South Wales Divi…\n 6 Bradfield 53681 34927       202         17261 New South Wales Divi…\n 7 Calare    54091 35779       285         25342 New South Wales Divi…\n 8 Chifley   32871 46702       263         28180 New South Wales Divi…\n 9 Cook      47505 38804       229         18713 New South Wales Divi…\n10 Cowper    57493 38317       315         25197 New South Wales Divi…\n# … with 140 more rows\n\nPivot_longer to recover structure\nSupposed we wanted to create a stacked bar chart to compare the % who votes Yes to the people who either said No or didn’t vote. The easiest way is to pivot longer into the original data format: State, Division, surveyResponse, count.\n\n\nvotes%>%\n  pivot_longer(\n    cols = Yes:`No Response`,\n    names_to = \"Response\",\n    values_to = \"Count\"\n  )\n\n\n# A tibble: 600 × 4\n   Town      Divisions                 Response    Count\n   <chr>     <chr>                     <chr>       <dbl>\n 1 Banks     New South Wales Divisions Yes         37736\n 2 Banks     New South Wales Divisions No          46343\n 3 Banks     New South Wales Divisions Illegible     247\n 4 Banks     New South Wales Divisions No Response 20928\n 5 Barton    New South Wales Divisions Yes         37153\n 6 Barton    New South Wales Divisions No          47984\n 7 Barton    New South Wales Divisions Illegible     226\n 8 Barton    New South Wales Divisions No Response 24008\n 9 Bennelong New South Wales Divisions Yes         42943\n10 Bennelong New South Wales Divisions No          43215\n# … with 590 more rows\n\nSpecial thanks to Karl, Shih-Yen, Mohit, and the other students in the advanced group for allowing me to use their blog submissions as a starting point for this demonstration!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-18-sathvikhotelbookingsdatahw3/",
    "title": "sathvik_hotel_bookings_data_hw3",
    "description": "Hotel bookings Data",
    "author": [
      {
        "name": "sathvik_thogaru",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\nImporting data\nThis data set contains a single file which compares various booking information between hotels.\n\n\nlibrary(skimr)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n\n\n\n\nhotel_bookings <- read_csv(\"../../_data/hotel_bookings.csv\")\nhead(hotel_bookings)\n\n\n# A tibble: 6 × 32\n  hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n  <chr>              <dbl>     <dbl>            <dbl> <chr>           \n1 Resort Hotel           0       342             2015 July            \n2 Resort Hotel           0       737             2015 July            \n3 Resort Hotel           0         7             2015 July            \n4 Resort Hotel           0        13             2015 July            \n5 Resort Hotel           0        14             2015 July            \n6 Resort Hotel           0        14             2015 July            \n# … with 27 more variables: arrival_date_week_number <dbl>,\n#   arrival_date_day_of_month <dbl>, stays_in_weekend_nights <dbl>,\n#   stays_in_week_nights <dbl>, adults <dbl>, children <dbl>,\n#   babies <dbl>, meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nskim() is an alternative to summary(), quickly providing a broad overview of a data frame. It handles data of all types, dispatching a different set of summary functions based on the types of columns in the data frame.\n\n\nskim(hotel_bookings)\n\n\nTable 1: Data summary\nName\nhotel_bookings\nNumber of rows\n119390\nNumber of columns\n32\n_______________________\n\nColumn type frequency:\n\ncharacter\n13\nDate\n1\nnumeric\n18\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nhotel\n0\n1\n10\n12\n0\n2\n0\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\nmeal\n0\n1\n2\n9\n0\n5\n0\ncountry\n0\n1\n2\n4\n0\n178\n0\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\nagent\n0\n1\n1\n4\n0\n334\n0\ncompany\n0\n1\n1\n4\n0\n353\n0\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\nreservation_status\n0\n1\n7\n9\n0\n3\n0\nVariable type: Date\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\nData Wrangling\ntidying the data\nfinding the na values in the dataframe( row and column) using which()\n\n\nwhich(is.na(hotel_bookings), arr.ind=TRUE)\n\n\n       row col\n[1,] 40601  11\n[2,] 40668  11\n[3,] 40680  11\n[4,] 41161  11\n\n\n\nhotel_bookings[c(40601,40668,40680,41161),]\n\n\n# A tibble: 4 × 32\n  hotel      is_canceled lead_time arrival_date_year arrival_date_mon…\n  <chr>            <dbl>     <dbl>             <dbl> <chr>            \n1 City Hotel           1         2              2015 August           \n2 City Hotel           1         1              2015 August           \n3 City Hotel           1         1              2015 August           \n4 City Hotel           1         8              2015 August           \n# … with 27 more variables: arrival_date_week_number <dbl>,\n#   arrival_date_day_of_month <dbl>, stays_in_weekend_nights <dbl>,\n#   stays_in_week_nights <dbl>, adults <dbl>, children <dbl>,\n#   babies <dbl>, meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\n\n\nhotel_bookings <- filter(hotel_bookings, !is.na(children)) \n\n\n\n\n\nsum(is.na(hotel_bookings))\n\n\n[1] 0\n\ntransforming data and visualizing\n\n\n(hotel_bookings<-hotel_bookings %>% \n  mutate(arrival_month = recode(arrival_date_month,\"January\"=1,\"February\"=2,\"March\"=3,\"April\"=4,\"May\"=5,\"June\"=6,\"July\"=7,\"August\"=8,\"September\"=9,\"October\"=10,\"November\"=11,\"December\"=12)))\n\n\n# A tibble: 119,386 × 33\n   hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n   <chr>              <dbl>     <dbl>            <dbl> <chr>           \n 1 Resort Hotel           0       342             2015 July            \n 2 Resort Hotel           0       737             2015 July            \n 3 Resort Hotel           0         7             2015 July            \n 4 Resort Hotel           0        13             2015 July            \n 5 Resort Hotel           0        14             2015 July            \n 6 Resort Hotel           0        14             2015 July            \n 7 Resort Hotel           0         0             2015 July            \n 8 Resort Hotel           0         9             2015 July            \n 9 Resort Hotel           1        85             2015 July            \n10 Resort Hotel           1        75             2015 July            \n# … with 119,376 more rows, and 28 more variables:\n#   arrival_date_week_number <dbl>, arrival_date_day_of_month <dbl>,\n#   stays_in_weekend_nights <dbl>, stays_in_week_nights <dbl>,\n#   adults <dbl>, children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\n\n\n(hotel_bookings <- hotel_bookings%>%\n  mutate(arrival_date = make_date(arrival_date_year,arrival_month,arrival_date_day_of_month)))\n\n\n# A tibble: 119,386 × 34\n   hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n   <chr>              <dbl>     <dbl>            <dbl> <chr>           \n 1 Resort Hotel           0       342             2015 July            \n 2 Resort Hotel           0       737             2015 July            \n 3 Resort Hotel           0         7             2015 July            \n 4 Resort Hotel           0        13             2015 July            \n 5 Resort Hotel           0        14             2015 July            \n 6 Resort Hotel           0        14             2015 July            \n 7 Resort Hotel           0         0             2015 July            \n 8 Resort Hotel           0         9             2015 July            \n 9 Resort Hotel           1        85             2015 July            \n10 Resort Hotel           1        75             2015 July            \n# … with 119,376 more rows, and 29 more variables:\n#   arrival_date_week_number <dbl>, arrival_date_day_of_month <dbl>,\n#   stays_in_weekend_nights <dbl>, stays_in_week_nights <dbl>,\n#   adults <dbl>, children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nlooking for type of hotels booked by guests\n\n\n(hotels_info<-data.frame(table(hotel_bookings$hotel)))\n\n\n          Var1  Freq\n1   City Hotel 79326\n2 Resort Hotel 40060\n\n\n\npie(hotels_info$Freq, labels=paste(hotels_info$Var1,sep = \"=\", hotels_info$Freq), main = \"type of hotels booked by guests\")\n\n\n\n\nwhere do most of the guests come from?\n\n\n(guests_country_details <- hotel_bookings %>% \n   group_by(country) %>% \n   count() %>% \n   ungroup() %>% \n   arrange(desc(n)))\n\n\n# A tibble: 178 × 2\n   country     n\n   <chr>   <int>\n 1 PRT     48586\n 2 GBR     12129\n 3 FRA     10415\n 4 ESP      8568\n 5 DEU      7287\n 6 ITA      3766\n 7 IRL      3375\n 8 BEL      2342\n 9 BRA      2224\n10 NLD      2104\n# … with 168 more rows\n\nmost guests by top country’s\n\n\nggplot(filter(guests_country_details, n>1500))+\n  geom_bar(aes(country, n), stat = \"identity\")+\n  labs(y=\"number of guests\", title = \"most guests by country's\")+\n  coord_flip()\n\n\n\n\nhow much do guests pay for a room at each hotel?\n\n\ncity_hotel_data<-filter(hotel_bookings, hotel_bookings$hotel==\"City Hotel\")\nresort_hotel_data<-filter(hotel_bookings, hotel_bookings$hotel==\"Resort Hotel\")\n\n\n\nadr - average daily rate\n\n\n# city hotel\nsort(unique(city_hotel_data$reserved_room_type))\n\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"P\"\n\n# resort hotel\nsort(unique(resort_hotel_data$reserved_room_type))\n\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"L\" \"P\"\n\ncity hotel average guests per room type\n\n\n(city_hotel_average_guests_per_room_type <- city_hotel_data %>% \n   group_by(reserved_room_type) %>% \n   summarise(guests_per_room_type = mean(adults+children)))\n\n\n# A tibble: 8 × 2\n  reserved_room_type guests_per_room_type\n  <chr>                             <dbl>\n1 A                                  1.82\n2 B                                  2.12\n3 C                                  1.64\n4 D                                  2.21\n5 E                                  2.34\n6 F                                  3.62\n7 G                                  3.29\n8 P                                  0   \n\ncity hotel average daily rate per room type\n\n\n(city_hotel_adr_per_room_type_data <- city_hotel_data %>% \n   group_by(reserved_room_type) %>% \n   summarise(adr_per_room_type = mean(adr)))\n\n\n# A tibble: 8 × 2\n  reserved_room_type adr_per_room_type\n  <chr>                          <dbl>\n1 A                               96.2\n2 B                               90.5\n3 C                               85.5\n4 D                              131. \n5 E                              157. \n6 F                              189. \n7 G                              202. \n8 P                                0  \n\nresort hotel average guests per room type\n\n\n(resort_average_guests_per_room_type <- resort_hotel_data %>% \n   group_by(reserved_room_type) %>% \n   summarise(guests_per_room_type = mean(adults+children)))\n\n\n# A tibble: 10 × 2\n   reserved_room_type guests_per_room_type\n   <chr>                             <dbl>\n 1 A                                  1.80\n 2 B                                  2   \n 3 C                                  3.34\n 4 D                                  2.00\n 5 E                                  1.99\n 6 F                                  2.05\n 7 G                                  3.37\n 8 H                                  3.69\n 9 L                                  2.17\n10 P                                  0   \n\n##resort hotel average daily rate per room type\n\n\n(resort_hotel_adr_per_room_type_data <- resort_hotel_data %>% \n   group_by(reserved_room_type) %>% \n   summarise(adr_per_room_type = mean(adr)))\n\n\n# A tibble: 10 × 2\n   reserved_room_type adr_per_room_type\n   <chr>                          <dbl>\n 1 A                               76.2\n 2 B                              105. \n 3 C                              161. \n 4 D                              104. \n 5 E                              114. \n 6 F                              133. \n 7 G                              168. \n 8 H                              188. \n 9 L                              125. \n10 P                                0  \n\nvisualizing Average daily rate of reserved room type at city hotel and resort hotel\n\n\nggplot(city_hotel_adr_per_room_type_data, aes(x=reserved_room_type, y= adr_per_room_type))+\n  geom_bar(stat = \"identity\",fill = \"steelblue\", width = .5)+\n  labs(x=\"reserved room type\", y=\" average daily rate per room\", title = \"Average daily rate of reserved room type at city hotel\")+\n  theme(plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\nggplot(resort_hotel_adr_per_room_type_data, aes(x=reserved_room_type, y= adr_per_room_type))+\n  geom_bar(stat = \"identity\",fill = \"steelblue\", width = .5)+\n  labs(x=\"reserved room type\", y=\" average daily rate per room\", title = \"Average daily rate of reserved room type at resort hotel\")+\n  theme(plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"))\n\n\n\n\nhow does the price per night varry in the year in each hotel?\ncity hotel\n\n\ncity_room_prices_monthly <-select(city_hotel_data, arrival_date_month, adr)\n(city_room_prices_monthly <- city_room_prices_monthly %>%\n    group_by(arrival_date_month) %>%\n    summarise(mean_room_prices = mean(adr)) %>% \n    ungroup() %>% \n  arrange(desc(mean_room_prices)))\n\n\n# A tibble: 12 × 2\n   arrival_date_month mean_room_prices\n   <chr>                         <dbl>\n 1 May                           122. \n 2 June                          119. \n 3 August                        115. \n 4 April                         111. \n 5 July                          111. \n 6 September                     110. \n 7 October                       100. \n 8 March                          92.6\n 9 December                       88.8\n10 November                       88.1\n11 February                       85.1\n12 January                        82.6\n\n\n\nplot <- ggplot(city_room_prices_monthly)+\n  geom_bar(aes(arrival_date_month, mean_room_prices),stat = \"identity\")+\n  labs(x=\"arrival_date_month\", y=\" mean_room_prices\", title = \"mean room prices over the year at city hotel\")+\n  theme(plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"))\n  \n\nplot +\n  scale_x_discrete(limits = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"))\n\n\n\n\nresort hotel\n\n\nresort_room_prices_monthly <-select(resort_hotel_data, arrival_date_month, adr)\n(resort_room_prices_monthly <- resort_room_prices_monthly %>%\n    group_by(arrival_date_month) %>%\n    summarise(mean_room_prices = mean(adr)) %>% \n    ungroup() %>% \n  arrange(desc(mean_room_prices)))\n\n\n# A tibble: 12 × 2\n   arrival_date_month mean_room_prices\n   <chr>                         <dbl>\n 1 August                        187. \n 2 July                          155. \n 3 June                          110. \n 4 September                      93.3\n 5 May                            78.8\n 6 April                          77.8\n 7 December                       69.0\n 8 October                        62.1\n 9 March                          57.5\n10 February                       55.2\n11 January                        49.5\n12 November                       48.3\n\n\n\nplot <- ggplot(resort_room_prices_monthly)+\n  geom_bar(aes(arrival_date_month, mean_room_prices),stat = \"identity\")+\n  labs(x=\"arrival_date_month\", y=\" mean_room_prices\", title = \"mean room prices over the year at resort hotel\")+\n  theme(plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"))\n  \n\nplot +\n  scale_x_discrete(limits = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"))\n\n\n\n\nwhich months are the busiest?\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-18-sathvikhotelbookingsdatahw3/sathvik_hotel_bookings_data_hw3_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2021-08-24T08:21:58-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-18-sathvikirisdatahw2/",
    "title": "sathvik_iris_data_hw2",
    "description": "the Iris dataset",
    "author": [
      {
        "name": "sathvik_thogaru",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\nloading the iris dataset from the datasets package and running summary statistics on the iris data\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nfinding the column nammes of the data\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n[5] \"Species\"     \n\ndim()is used to find the dimensions of the data\n\n[1] 150   5\n\nskim() is an alternative to summary(), quickly providing a broad overview of a data frame. It handles data of all types, dispatching a different set of summary functions based on the types of columns in the data frame.\n\nTable 1: Data summary\nName\niris\nNumber of rows\n150\nNumber of columns\n5\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n4\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\nthere are 4 numeric variables and 1 factor variable which is the species. there are a total of 3 unique species in the iris\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\nthe three species are setosa, versicolor and virginica\nploting iris\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-18-sathvikirisdatahw2/sathvik_iris_data_hw2_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-08-24T08:22:01-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-18-sathvikthogaruhomework4/",
    "title": "sathvik_thogaru_homework4",
    "description": "hotel_bookings Dataset",
    "author": [
      {
        "name": "sathvik_thogaru",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\nImporting data\nThis data set contains a single file which compares various booking information between hotels.\n\n\n\nimporting the data and reading the top 5 rows\n\n# A tibble: 6 × 32\n  hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n  <chr>              <dbl>     <dbl>            <dbl> <chr>           \n1 Resort Hotel           0       342             2015 July            \n2 Resort Hotel           0       737             2015 July            \n3 Resort Hotel           0         7             2015 July            \n4 Resort Hotel           0        13             2015 July            \n5 Resort Hotel           0        14             2015 July            \n6 Resort Hotel           0        14             2015 July            \n# … with 27 more variables: arrival_date_week_number <dbl>,\n#   arrival_date_day_of_month <dbl>, stays_in_weekend_nights <dbl>,\n#   stays_in_week_nights <dbl>, adults <dbl>, children <dbl>,\n#   babies <dbl>, meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nskim() is used to for getting summary statistics about variables in dataframe,tibbles,datatablesand vectors. It is mostly used with grouped dataframes (source: https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html)\n\nTable 1: Data summary\nName\nhotel_bookings\nNumber of rows\n119390\nNumber of columns\n32\n_______________________\n\nColumn type frequency:\n\ncharacter\n13\nDate\n1\nnumeric\n18\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nhotel\n0\n1\n10\n12\n0\n2\n0\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\nmeal\n0\n1\n2\n9\n0\n5\n0\ncountry\n0\n1\n2\n4\n0\n178\n0\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\nagent\n0\n1\n1\n4\n0\n334\n0\ncompany\n0\n1\n1\n4\n0\n353\n0\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\nreservation_status\n0\n1\n7\n9\n0\n3\n0\nVariable type: Date\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\nfrom the above summary statistics we can see there are a total of 119390 rows and 32 columns in the hotel_bookings dataset. 13 character variables, 18 numeric variables, and 1 date variable. there are a total of 4 missing values in the children variable. for the analysis now i will be using hotel, market segment, stays_in_weekend_nights and stays_in_week_nights.\nVaraible Description\nhotel variable: type of hotel booked\nmarket segment : Market segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\nstays_in_weekend_nights : guest stayed at the hotel in weekend nights\nstays_in_week_nights : guest stayed at the hotel in week nights\nI am using the select() from the dplyr package which comes with tidyverse package and the piping for selecting columns\n\n# A tibble: 119,390 × 4\n   hotel        stays_in_weekend_ni… stays_in_week_nig… market_segment\n   <chr>                       <dbl>              <dbl> <chr>         \n 1 Resort Hotel                    0                  0 Direct        \n 2 Resort Hotel                    0                  0 Direct        \n 3 Resort Hotel                    0                  1 Direct        \n 4 Resort Hotel                    0                  1 Corporate     \n 5 Resort Hotel                    0                  2 Online TA     \n 6 Resort Hotel                    0                  2 Online TA     \n 7 Resort Hotel                    0                  2 Direct        \n 8 Resort Hotel                    0                  2 Direct        \n 9 Resort Hotel                    0                  3 Online TA     \n10 Resort Hotel                    0                  3 Offline TA/TO \n# … with 119,380 more rows\n\n\n[1] \"Resort Hotel\" \"City Hotel\"  \n[1] \"Direct\"        \"Corporate\"     \"Online TA\"     \"Offline TA/TO\"\n[5] \"Complementary\" \"Groups\"        \"Undefined\"     \"Aviation\"     \n\n\n\n\nbookings in different market segments\ncity hotel\n\n# A tibble: 8 × 2\n  market_segment     n\n  <chr>          <int>\n1 Aviation         237\n2 Complementary    542\n3 Corporate       2986\n4 Direct          6093\n5 Groups         13975\n6 Offline TA/TO  16747\n7 Online TA      38748\n8 Undefined          2\n\n\nresort hotel\n\n# A tibble: 6 × 2\n  market_segment     n\n  <chr>          <int>\n1 Complementary    201\n2 Corporate       2309\n3 Direct          6513\n4 Groups          5836\n5 Offline TA/TO   7472\n6 Online TA      17729\n\n\n\n# A tibble: 14 × 3\n# Groups:   hotel, market_segment [14]\n   hotel        market_segment     n\n   <chr>        <chr>          <int>\n 1 City Hotel   Aviation         237\n 2 City Hotel   Complementary    542\n 3 City Hotel   Corporate       2986\n 4 City Hotel   Direct          6093\n 5 City Hotel   Groups         13975\n 6 City Hotel   Offline TA/TO  16747\n 7 City Hotel   Online TA      38748\n 8 City Hotel   Undefined          2\n 9 Resort Hotel Complementary    201\n10 Resort Hotel Corporate       2309\n11 Resort Hotel Direct          6513\n12 Resort Hotel Groups          5836\n13 Resort Hotel Offline TA/TO   7472\n14 Resort Hotel Online TA      17729\n\n\n\n\nhow many number of days do people stay in the hotel?\nResort hotel\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-18-sathvikthogaruhomework4/sathvik_thogaru_hw4_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-08-24T08:22:10-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-16-railroad-employment-data/",
    "title": "Railroad Employment data",
    "description": "Reading in the railroad dataset",
    "author": [
      {
        "name": "Mohit-Arora",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\nlibrary(readxl)\n  library(dplyr)\n  State_county <- read_excel(\"../../_data/StateCounty2012.xls\", skip=3)\n  \n\n\nThe data has been read in. Let’s see how it looks like -\n\n\nhead(State_county)\n\n\n# A tibble: 6 × 5\n  STATE     ...2  COUNTY               ...4  TOTAL\n  <chr>     <lgl> <chr>                <lgl> <dbl>\n1 AE        NA    APO                  NA        2\n2 AE Total1 NA    <NA>                 NA        2\n3 AK        NA    ANCHORAGE            NA        7\n4 AK        NA    FAIRBANKS NORTH STAR NA        2\n5 AK        NA    JUNEAU               NA        3\n6 AK        NA    MATANUSKA-SUSITNA    NA        2\n\ntail(State_county)\n\n\n# A tibble: 6 × 5\n  STATE                                       ...2  COUNTY ...4  TOTAL\n  <chr>                                       <lgl> <chr>  <lgl> <dbl>\n1 <NA>                                        NA    <NA>   NA       NA\n2 CANADA                                      NA    <NA>   NA      662\n3 <NA>                                        NA    <NA>   NA       NA\n4 1  Military designation.                    NA    <NA>   NA       NA\n5 <NA>                                        NA    <NA>   NA       NA\n6 NOTE:  Excludes 2,896 employees without an… NA    <NA>   NA       NA\n\nThis needs further cleaning to make it suitable for further analysis.\n\n\nState_county <- select(State_county, -c(2, 4))\nState_county <- State_county[complete.cases(State_county),]\nhead(State_county)\n\n\n# A tibble: 6 × 3\n  STATE COUNTY               TOTAL\n  <chr> <chr>                <dbl>\n1 AE    APO                      2\n2 AK    ANCHORAGE                7\n3 AK    FAIRBANKS NORTH STAR     2\n4 AK    JUNEAU                   3\n5 AK    MATANUSKA-SUSITNA        2\n6 AK    SITKA                    1\n\ntail(State_county)\n\n\n# A tibble: 6 × 3\n  STATE COUNTY     TOTAL\n  <chr> <chr>      <dbl>\n1 WY    SHERIDAN     252\n2 WY    SUBLETTE       3\n3 WY    SWEETWATER   196\n4 WY    UINTA         49\n5 WY    WASHAKIE      10\n6 WY    WESTON        37\n\nThis looks much better!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:43-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-dn-australian-data/",
    "title": "DN Australian Data",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Dana Nestor",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\nImporting data using a specific range to isolate desired variables, renaming variables to add meaningful values and allow for easier selecting, removing interstitial undesirable variables\n\n\nbase_marriage_data <- read_excel(\"../../_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\", \n         sheet = \"Table 2\", \n         range = \"A8:P179\", \n         col_names = c(\"Town\", \"Yes\", \"d\", \"No\", rep(\"d\", 6), \"Not Clear\", \"d\", \n                       \"No Response\", rep(\"d\", 3)))%>%\n  select(!starts_with(\"d\"))\n\n\n\nThe next variable we need to isolate is County which, in this data set, has a parent-child relationship with Town. To accomplish this, we will create a new column for County values that correlate with their child towns and order the columns in descending complexity (in this case, county then town)\n\n\nbase_marriage_data <- base_marriage_data%>%\n  mutate(County = case_when(\n    str_ends(Town, \"Divisions\") ~ Town, \n    TRUE ~ NA_character_))%>%\n# Because I cannot get the .before or .after arguments to work with mutate(), I am using the relocate() function to move the County column before the Town column so we can maintain a descending order of complexity in governmental organizations\n  relocate(County, .before = Town)\n\n\n\nTo complete the isolation of County data, we need to populate our new column with the appropriate parent-county for their associated child-towns. We use a loop function to pull the County value down our column, stopping when a new county is reached and then restarting itself with the new county value.\n\n\ntidy_marriage_data <- base_marriage_data\nfor(i in seq_along(tidy_marriage_data$County)) {tidy_marriage_data$County[i] <- ifelse(is.na(tidy_marriage_data$County[i]), tidy_marriage_data$County[i-1], tidy_marriage_data$County[i])}\n\n\n\nThis next chunk removes undesirable rows so we can isolate our observations. Since we were able to import our data with a range that cut out unnecessary rows above and below our data frame, now we need to account for interstitial rows without data and rows with totals\n\n\ntidier_marriage_data <- tidy_marriage_data%>%drop_na(Town, Yes)%>%\n  filter(!str_detect(Town, \"(Total)\"))\n\n\n\nAs an extra step to tidy the data, we can remove “Divisions” in the County column as this variable is now describing the county itself, not the child-towns.\n\n\ntidiest_marriage_data <- mutate(tidier_marriage_data, County = str_remove(County, \" Divisions\"))%>%\n  mutate(Town = str_remove(Town, \"\\\\([cde]\\\\)\"))\n  view(tidiest_marriage_data)\n\n\n\nNOTE tidy objects, condense code (under 15 lines?)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-example-code-for-pivot-longer/",
    "title": "Example Code for Pivot Longer",
    "description": "I'm sharing some example code for pivot_longer using the eggs data. Enjoy!",
    "author": [
      {
        "name": "Meredith Rolfe",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [
      "example code",
      "data cleaning"
    ],
    "contents": "\nThis is example code for using the pivot functions in R. Several of the government data sources include tabular data that really need to be pivoted into a dataset in which a “case” is some combination of the grouping variables (the rows and columns in the table) alongside the appropriate statistical value(s) in the table (e.g., counts or average costs). Lets start with the easy to read in eggs_tidy.csv just so we can focus on the pivoting function.\n\n\neggs<-read_csv(\"../../_data/eggs_tidy.csv\", show_col_types = FALSE)\neggs\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>\n 1 January    2004             126         230                    132 \n 2 February   2004             128.        226.                   134.\n 3 March      2004             131         225                    137 \n 4 April      2004             131         225                    137 \n 5 May        2004             131         225                    137 \n 6 June       2004             134.        231.                   137 \n 7 July       2004             134.        234.                   137 \n 8 August     2004             134.        234.                   137 \n 9 September  2004             130.        234.                   136.\n10 October    2004             128.        234.                   136.\n# … with 110 more rows, and 1 more variable: extra_large_dozen <dbl>\n\nLooking at the data, we can see that each case consists of a year-month combination (e.g., January 2004), while the values are the average price (in cents) of four different types of eggs (e.g., large_half_dozen, large_dozen, etc) But really, wouldn’t it possibly make more sense to consider the case as a year-month-type combination, with a single price value for each case?\nPivot Longer - One New Category Variable\nTo do this (and make our data easier to graph and analyze), we can pivot longer - changing our data from 120 rows with 6 variables (2 grouping and 4 values) to 480 rows of 4 variables (with 3 grouping variables and a single price value).\n\n\neggs%>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen, \n               names_to = \"eggType\",\n               values_to = \"avgPrice\"\n  )\n\n\n# A tibble: 480 × 4\n   month     year eggType                avgPrice\n   <chr>    <dbl> <chr>                     <dbl>\n 1 January   2004 large_half_dozen           126 \n 2 January   2004 large_dozen                230 \n 3 January   2004 extra_large_half_dozen     132 \n 4 January   2004 extra_large_dozen          230 \n 5 February  2004 large_half_dozen           128.\n 6 February  2004 large_dozen                226.\n 7 February  2004 extra_large_half_dozen     134.\n 8 February  2004 extra_large_dozen          230 \n 9 March     2004 large_half_dozen           131 \n10 March     2004 large_dozen                225 \n# … with 470 more rows\n\nWell, that was super easy. But wait, what if you are interested in egg size - you want to know how much more expensive extra-large eggs are compared to large eggs. Right now, that will be annoying, as you will have to keep sorting out the egg quantity - whether the price is for a half_dozen or a dozen eggs. Wouldn’t it be nice if we didn’t have a long egg type column with both size and quantity squashed into a single categorical variable? It would be so useful to have a new dataset with 4 grouping variables (year, month, size, and quantity) and the same value (price).\nPivot Longer - Two New Category Variables\nSo, once again we want to use pivot longer, but we will be adding two new category variables (for a total of 4) and this will cut the number of rows in half (to 240). But how in the world can we let R know what we want it to do?? Thankfully, someone named the egg types (column-names) pretty systematically, but how can use this to our advantage? Working with patterns in the names_sep option of the pivot functions makes it pretty easy (well, except our variable names have more than one underscore, so we have to sort of hack this part by also using mutate on the resulting category labels.)\n\n\neggs%>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"size\", \"quantity\"),\n               names_sep=\"arge_\",\n               values_to = \"price\"\n  ) %>%\n  mutate(size = case_when(\n    size == \"l\" ~ \"Large\",\n    size == \"extra_l\" ~ \"Extra Large\"\n  ))\n\n\n# A tibble: 480 × 5\n   month     year size        quantity   price\n   <chr>    <dbl> <chr>       <chr>      <dbl>\n 1 January   2004 Large       half_dozen  126 \n 2 January   2004 Large       dozen       230 \n 3 January   2004 Extra Large half_dozen  132 \n 4 January   2004 Extra Large dozen       230 \n 5 February  2004 Large       half_dozen  128.\n 6 February  2004 Large       dozen       226.\n 7 February  2004 Extra Large half_dozen  134.\n 8 February  2004 Extra Large dozen       230 \n 9 March     2004 Large       half_dozen  131 \n10 March     2004 Large       dozen       225 \n# … with 470 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-homework-2/",
    "title": "School Characteristics",
    "description": "Blog posts about School Characteristics",
    "author": [
      {
        "name": "Joey Wolpert",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\nHomework 2\nLet’s read in the data and see what it looks like:\n\n          X        Y OBJECTID    NCESSCH\n1 -149.3578 61.62714        1 2.0051e+10\n2 -156.7542 71.30034        2 2.0061e+10\n3 -151.0701 60.49144        3 2.0039e+10\n4 -151.2791 60.56828        4 2.0039e+10\n5 -151.2323 60.56700        5 2.0039e+10\n6 -133.1287 56.11917        6 2.0070e+10\n                             NMCNTY  SURVYEAR STABR  LEAID ST_LEAID\n1         Matanuska-Susitna Borough 2017-2018    AK 200510    AK-33\n2               North Slope Borough 2017-2018    AK 200610    AK-36\n3           Kenai Peninsula Borough 2017-2018    AK 200390    AK-24\n4           Kenai Peninsula Borough 2017-2018    AK 200390    AK-24\n5           Kenai Peninsula Borough 2017-2018    AK 200390    AK-24\n6 Prince of Wales-Hyder Census Area 2017-2018    AK 200700    AK-44\n                                   LEA_NAME\n1 Matanuska-Susitna Borough School District\n2       North Slope Borough School District\n3   Kenai Peninsula Borough School District\n4   Kenai Peninsula Borough School District\n5   Kenai Peninsula Borough School District\n6          Southeast Island School District\n                               SCH_NAME           LSTREET1 LSTREET2\n1                  John Shaw Elementary 3750 E Paradise Ln         \n2              Kiita Learning Community     5246 Karluk St         \n3    Soldotna Montessori Charter School     158 E Park Ave         \n4 Kaleidoscope School of Arts & Science    549 N Forest Dr         \n5                       Marathon School    405 Marathon Rd         \n6                     Whale Pass School     126 Bayview Rd         \n  LSTREET3      LCITY LSTATE  LZIP LZIP4         PHONE GSLO GSHI\n1             Wasilla     AK 99654    NA (907)352-0500   PK   05\n2           Utqiagvik     AK 99723    NA (907)852-9677   09   12\n3            Soldotna     AK 99669    NA (907)260-9221   KG   06\n4               Kenai     AK 99611    NA (907)283-0804   KG   05\n5               Kenai     AK 99611    NA (907)335-3343   07   12\n6          Whale Pass     AK 99950    NA (907)846-5320   PK   12\n               VIRTUAL TOTFRL FRELCH REDLCH PK KG G01 G02 G03 G04 G05\n1 Not a virtual school    183    158     25 30 81  63  80  62  58  73\n2 Not a virtual school     27     27      0 NA NA  NA  NA  NA  NA  NA\n3 Not a virtual school     43     23     20 NA 23  23  27  22  25  28\n4 Not a virtual school     69     50     19 NA 40  43  42  46  46  43\n5 Not a virtual school     -9     -9     -9 NA NA  NA  NA  NA  NA  NA\n6 Not a virtual school     17     17      0  0  0   3   1   2   2   2\n  G06 G07 G08 G09 G10 G11 G12 G13 TOTAL MEMBER AM HI BL  WH HP TR\n1  NA  NA  NA  NA  NA  NA  NA  NA   447    447 50 12  5 351  2 23\n2  NA  NA  NA   0   3   7  20  NA    30     30 27  0  0   0  1  2\n3  19  NA  NA  NA  NA  NA  NA  NA   167    167  8  5  0 136  0 15\n4  NA  NA  NA  NA  NA  NA  NA  NA   260    260 16 14  3 168  0 56\n5  NA   0   1   1   2   1   0  NA     5      5  0  0  1   3  1  0\n6   1   5   1   0   0   0   1  NA    18     18  0  1  0  13  0  4\n    FTE   LATCOD    LONCOD          ULOCALE STUTERATIO        STITLEI\n1 24.90 61.62714 -149.3579 41-Rural: Fringe      17.95            Yes\n2  3.00 71.30034 -156.7542  33-Town: Remote      10.00 Not Applicable\n3 10.35 60.49144 -151.0702  33-Town: Remote      16.14 Not Applicable\n4 16.75 60.56828 -151.2791  33-Town: Remote      15.52 Not Applicable\n5  0.67 60.56700 -151.2323  33-Town: Remote       7.46            Yes\n6  1.90 56.11918 -133.1287 43-Rural: Remote       9.47            Yes\n  AMALM AMALF ASALM ASALF HIALM HIALF BLALM BLALF WHALM WHALF HPALM\n1    33    17     1     3    10     2     3     2   193   158     0\n2    16    11     0     0     0     0     0     0     0     0     1\n3     4     4     0     3     2     3     0     0    58    78     0\n4    10     6     1     2     6     8     3     0    82    86     0\n5     0     0     0     0     0     0     0     1     1     2     0\n6     0     0     0     0     1     0     0     0     5     8     0\n  HPALF TRALM TRALF TOTMENROL TOTFENROL STATUS UG AE\n1     2    11    12       251       196      1 NA NA\n2     0     1     1        18        12      1 NA NA\n3     0     7     8        71        96      1 NA NA\n4     0    26    30       128       132      1 NA NA\n5     1     0     0         1         4      1 NA NA\n6     0     4     0        10         8      1 NA NA\n          SCHOOL_TYPE_TEXT         SY_STATUS_TEXT SCHOOL_LEVEL AS\n1           Regular school Currently operational    Elementary  4\n2 Alternative/other school Currently operational          High  0\n3           Regular school Currently operational    Elementary  3\n4           Regular school Currently operational    Elementary  3\n5 Alternative/other school Currently operational          High  0\n6           Regular school Currently operational         Other  0\n  CHARTER_TEXT MAGNET_TEXT\n1           No          No\n2           No          No\n3          Yes          No\n4          Yes          No\n5           No          No\n6           No          No\n\nWhat’s in the data?\nThis dataset looks at the characteristics of various public schools across the United States. Among the variables in the dataset are identifying characteristics such as the name of the school, its school district, and its location; there are also several quantitative variables such as the number of students in each grade, as well as the overall number of students broken down in categories such as race and gender.\nThe numbers of the data\nThe data has 79 total columns and just over 100,000 rows. It is unlikely that all of this info will be useful, so in the next section we can see if the data can be cleaned and subset to be more useful to the project.\nHomework 3\nCleaning Data\nThe data is definitely unclean, let’s filter out some stuff so that we have more complete data.\n\n          X        Y OBJECTID     NCESSCH                     NMCNTY\n1 -151.0701 60.49144        3 20039000448    Kenai Peninsula Borough\n2 -151.2791 60.56828        4 20039000463    Kenai Peninsula Borough\n3 -166.5224 53.86895       15 20072000340 Aleutians West Census Area\n4 -166.5296 53.87267       16 20072000661 Aleutians West Census Area\n5 -161.7707 60.80436       18 20000100207         Bethel Census Area\n6 -161.7704 60.80258       19 20000100208         Bethel Census Area\n   SURVYEAR STABR  LEAID ST_LEAID\n1 2017-2018    AK 200390    AK-24\n2 2017-2018    AK 200390    AK-24\n3 2017-2018    AK 200720    AK-47\n4 2017-2018    AK 200720    AK-47\n5 2017-2018    AK 200001    AK-31\n6 2017-2018    AK 200001    AK-31\n                                 LEA_NAME\n1 Kenai Peninsula Borough School District\n2 Kenai Peninsula Borough School District\n3           Unalaska City School District\n4           Unalaska City School District\n5         Lower Kuskokwim School District\n6         Lower Kuskokwim School District\n                               SCH_NAME                     LSTREET1\n1    Soldotna Montessori Charter School               158 E Park Ave\n2 Kaleidoscope School of Arts & Science              549 N Forest Dr\n3        Eagle's View Elementary School            503 East Broadway\n4            Unalaska Jr/Sr High School                55 E Broadway\n5                Gladys Jung Elementary 1007 Ron Edwards Memorial Dr\n6           Bethel Regional High School 1006 Ron Edwards Memorial Dr\n  LSTREET2 LSTREET3    LCITY LSTATE  LZIP LZIP4         PHONE GSLO\n1                   Soldotna     AK 99669    NA (907)260-9221   KG\n2                      Kenai     AK 99611    NA (907)283-0804   KG\n3                   Unalaska     AK 99685    NA (907)581-3979   PK\n4                   Unalaska     AK 99685    NA (907)581-1222   07\n5                     Bethel     AK 99559    NA (907)543-4440   03\n6                     Bethel     AK 99559    NA (907)543-3957   07\n  GSHI              VIRTUAL TOTFRL FRELCH REDLCH PK KG G01 G02 G03\n1   06 Not a virtual school     43     23     20 NA 23  23  27  22\n2   05 Not a virtual school     69     50     19 NA 40  43  42  46\n3   06 Not a virtual school     53     35     18  0 32  30  36  33\n4   12 Not a virtual school     38     27     11 NA NA  NA  NA  NA\n5   06 Not a virtual school    294    294      0 NA NA  NA  NA  97\n6   12 Not a virtual school    373    373      0 NA NA  NA  NA  NA\n  G04 G05 G06 G07 G08 G09 G10 G11 G12 G13 TOTAL MEMBER  AM HI BL  WH\n1  25  28  19  NA  NA  NA  NA  NA  NA  NA   167    167   8  5  0 136\n2  46  43  NA  NA  NA  NA  NA  NA  NA  NA   260    260  16 14  3 168\n3  31  26  29  NA  NA  NA  NA  NA  NA  NA   217    217  23 30  2  56\n4  NA  NA  NA  30  25  26  38  36  29  NA   184    184  24 21  0  38\n5  75  79  90  NA  NA  NA  NA  NA  NA  NA   341    341 284  6  1  44\n6  NA  NA  NA  94  90 106  52  63  70  NA   475    475 418  7  1  38\n  HP TR   FTE   LATCOD    LONCOD          ULOCALE STUTERATIO\n1  0 15 10.35 60.49144 -151.0702  33-Town: Remote      16.14\n2  0 56 16.75 60.56828 -151.2791  33-Town: Remote      15.52\n3 13  3 13.50 53.86895 -166.5225 43-Rural: Remote      16.07\n4  8  0 14.50 53.87267 -166.5296 43-Rural: Remote      12.69\n5  0  1 22.13 60.80436 -161.7707 41-Rural: Fringe      15.41\n6  0  2 33.05 60.80258 -161.7704 41-Rural: Fringe      14.37\n         STITLEI AMALM AMALF ASALM ASALF HIALM HIALF BLALM BLALF\n1 Not Applicable     4     4     0     3     2     3     0     0\n2 Not Applicable    10     6     1     2     6     8     3     0\n3 Not Applicable    11    12    52    38    14    16     0     2\n4 Not Applicable    12    12    52    41    12     9     0     0\n5            Yes   141   143     2     3     4     2     0     1\n6            Yes   221   197     5     4     4     3     0     1\n  WHALM WHALF HPALM HPALF TRALM TRALF TOTMENROL TOTFENROL STATUS UG\n1    58    78     0     0     7     8        71        96      1 NA\n2    82    86     0     0    26    30       128       132      1 NA\n3    26    30     7     6     1     2       111       106      1 NA\n4    23    15     4     4     0     0       103        81      1 NA\n5    21    23     0     0     0     1       168       173      1 NA\n6    20    18     0     0     1     1       251       224      1 NA\n  AE SCHOOL_TYPE_TEXT         SY_STATUS_TEXT SCHOOL_LEVEL AS\n1 NA   Regular school Currently operational    Elementary  3\n2 NA   Regular school Currently operational    Elementary  3\n3 NA   Regular school Currently operational    Elementary 90\n4 NA   Regular school Currently operational          High 93\n5 NA   Regular school Currently operational    Elementary  5\n6 NA   Regular school Currently operational          High  9\n  CHARTER_TEXT MAGNET_TEXT\n1          Yes          No\n2          Yes          No\n3           No          No\n4           No          No\n5           No          No\n6           No          No\n\nWith filtering, we now have a subset of the original data that will be much more useful for analysis. The schools were filtered to include only regular, non-virtual schools at the elementary, middle, and high school levels. It also removed any schools that had students younger than kindergarten (PK) or those past their senior year of high school (G13).\nSubsetting Columns\nNow let’s subset the columns in the dataset to only include those that interest this project.\n\n                               SCH_NAME STABR GSLO GSHI G01 G02 G03\n1    Soldotna Montessori Charter School    AK   KG   06  23  27  22\n2 Kaleidoscope School of Arts & Science    AK   KG   05  43  42  46\n3        Eagle's View Elementary School    AK   PK   06  30  36  33\n4            Unalaska Jr/Sr High School    AK   07   12  NA  NA  NA\n5                Gladys Jung Elementary    AK   03   06  NA  NA  97\n6           Bethel Regional High School    AK   07   12  NA  NA  NA\n  G04 G05 G06 G07 G08 G09 G10 G11 G12 TOTAL  AM HI BL  WH HP TR   FTE\n1  25  28  19  NA  NA  NA  NA  NA  NA   167   8  5  0 136  0 15 10.35\n2  46  43  NA  NA  NA  NA  NA  NA  NA   260  16 14  3 168  0 56 16.75\n3  31  26  29  NA  NA  NA  NA  NA  NA   217  23 30  2  56 13  3 13.50\n4  NA  NA  NA  30  25  26  38  36  29   184  24 21  0  38  8  0 14.50\n5  75  79  90  NA  NA  NA  NA  NA  NA   341 284  6  1  44  0  1 22.13\n6  NA  NA  NA  94  90 106  52  63  70   475 418  7  1  38  0  2 33.05\n  STUTERATIO AMALM AMALF ASALM ASALF HIALM HIALF BLALM BLALF WHALM\n1      16.14     4     4     0     3     2     3     0     0    58\n2      15.52    10     6     1     2     6     8     3     0    82\n3      16.07    11    12    52    38    14    16     0     2    26\n4      12.69    12    12    52    41    12     9     0     0    23\n5      15.41   141   143     2     3     4     2     0     1    21\n6      14.37   221   197     5     4     4     3     0     1    20\n  WHALF HPALM HPALF TRALM TRALF TOTMENROL TOTFENROL SCHOOL_LEVEL\n1    78     0     0     7     8        71        96   Elementary\n2    86     0     0    26    30       128       132   Elementary\n3    30     7     6     1     2       111       106   Elementary\n4    15     4     4     0     0       103        81         High\n5    23     0     0     0     1       168       173   Elementary\n6    18     0     0     1     1       251       224         High\n\nArranging Some Data\nIt may be useful to be the top several rows for certain columns, in this script, we look at the top 6 schools ordered based on their student to teacher ratio.\n\n                                SCH_NAME STABR GSLO GSHI G01 G02 G03\n1                      MARMOT SCHOOL K-8    ND   KG   08   0   0   0\n2 Kalamazoo Area Math and Science Center    MI   09   12  NA  NA  NA\n3          Woodruff Career & Tech Center    IL   05   12  NA  NA  NA\n4                        South Fork High    CA   09   12  NA  NA  NA\n5             Snohomish Detention Center    WA   06   12  NA  NA  NA\n6                      Cold Springs High    CA   09   12  NA  NA  NA\n  G04 G05 G06 G07 G08 G09 G10 G11 G12 TOTAL AM HI BL WH HP TR   FTE\n1   0   0   0   1   2  NA  NA  NA  NA     3  3  0  0  0  0  0 16.00\n2  NA  NA  NA  NA  NA   1   0   1   0     2  0  0  0  2  0  0  9.50\n3  NA  NA  NA  NA  NA   1  NA   1   1     3 NA NA  3 NA NA NA 10.00\n4  NA  NA  NA  NA  NA   0   0   0   1     1  1  0  0  0  0  0  2.00\n5  NA  NA   0   0   0   0   0   1   0     1  0  0  0  1  0  0  2.00\n6  NA  NA  NA  NA  NA   0   1   0   0     1  0  0  0  1  0  0  1.57\n  STUTERATIO AMALM AMALF ASALM ASALF HIALM HIALF BLALM BLALF WHALM\n1       0.19     1     2     0     0     0     0     0     0     0\n2       0.21     0     0     0     0     0     0     0     0     1\n3       0.30    NA    NA    NA    NA    NA    NA     2     1    NA\n4       0.50     1     0     0     0     0     0     0     0     0\n5       0.50     0     0     0     0     0     0     0     0     1\n6       0.64     0     0     0     0     0     0     0     0     1\n  WHALF HPALM HPALF TRALM TRALF TOTMENROL TOTFENROL SCHOOL_LEVEL\n1     0     0     0     0     0         1         2   Elementary\n2     1     0     0     0     0         1         1         High\n3    NA    NA    NA    NA    NA         2         1         High\n4     0     0     0     0     0         1         0         High\n5     0     0     0     0     0         1         0         High\n6     0     0     0     0     0         1         0         High\n\nAnd now we look at the bottom 6 schools with the highest student-teacher ratio.\n\n                                             SCH_NAME STABR GSLO GSHI\n59238                      RIVER CITY SCIENCE ACADEMY    FL   06   12\n59239                     DUVAL CHARTER AT BAYMEADOWS    FL   KG   08\n59240        Pathways Early College Innovation School    MA   11   12\n59241             ENERGIZED FOR EXCELLENCE ACADEMY EL    TX   KG   05\n59242 Gateway to College at Holyoke Community College    MA   10   12\n59243                          The Gateway to College    MA   09   12\n      G01 G02 G03 G04 G05 G06 G07 G08 G09 G10 G11 G12 TOTAL AM   HI\n59238  NA  NA  NA  NA  NA 150 152 142 137 147 114 100   942  7  155\n59239 138 132 133 130 125 154 153 156  NA  NA  NA  NA  1252  7  176\n59240  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  17  19    36  0    2\n59241 315 310 286 285 281  NA  NA  NA  NA  NA  NA  NA  1802  6 1585\n59242  NA  NA  NA  NA  NA  NA  NA  NA  NA   6  11   9    26  0   12\n59243  NA  NA  NA  NA  NA  NA  NA  NA  40  NA  17  26    83  0   15\n       BL  WH HP TR  FTE STUTERATIO AMALM AMALF ASALM ASALF HIALM\n59238 202 483  1 45 1.00        942     7     0    18    31    82\n59239 287 564  5 63 1.00       1252     2     5    75    75    88\n59240   0  30  0  2 0.02       1800     0     0     0     2     1\n59241 191   9  0  0 1.00       1802     5     1     2     9   788\n59242   8   6  0  0 0.01       2600     0     0     0     0     4\n59243   1  63  0  3 0.02       4150     0     0     0     1     4\n      HIALF BLALM BLALF WHALM WHALF HPALM HPALF TRALM TRALF TOTMENROL\n59238    73   100   102   265   218     1     0    21    24       494\n59239    88   152   135   284   280     3     2    37    26       641\n59240     1     0     0     8    22     0     0     1     1        10\n59241   797   102    89     4     5     0     0     0     0       901\n59242     8     5     3     1     5     0     0     0     0        10\n59243    11     0     1    28    35     0     0     0     3        32\n      TOTFENROL SCHOOL_LEVEL\n59238       448         High\n59239       611   Elementary\n59240        26         High\n59241       901   Elementary\n59242        16         High\n59243        51         High\n\nSummary Data\nNow let’s take a look at some summary of the dataset in terms of its student-teacher ratio in each state.\n\n# A tibble: 53 × 3\n# Groups:   STABR [53]\n   STABR AvgRatio SDRatio\n   <chr>    <dbl>   <dbl>\n 1 AK        16.9    8.97\n 2 AL        18.2    3.48\n 3 AR        13.2    2.83\n 4 AZ        19.1    4.90\n 5 CA        23.7    9.99\n 6 CO        16.6    4.83\n 7 CT        13.7   26.4 \n 8 DC        20.9   30.3 \n 9 DE        14.9    2.63\n10 FL        21.0   50.2 \n# … with 43 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:02-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-hw02/",
    "title": "HW02",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "TMoraitis",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n#Loading Library\nlibrary(tidyverse)\n\n\n\n##Data\n\n\npoultry <- read_csv(file=\"../../_data/poultry_tidy.csv\")\nhead(poultry)\n\n\n# A tibble: 6 × 4\n  Product  Year Month    Price_Dollar\n  <chr>   <dbl> <chr>           <dbl>\n1 Whole    2013 January          2.38\n2 Whole    2013 February         2.38\n3 Whole    2013 March            2.38\n4 Whole    2013 April            2.38\n5 Whole    2013 May              2.38\n6 Whole    2013 June             2.38\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:04-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-hw03/",
    "title": "HW03",
    "description": "This is my submission of HW3",
    "author": [],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n#Loading Library\nlibrary(tidyverse)\n\n\n\n\n\n# Reading file\nhotelBookings <- read_csv(file=\"../../_data/hotel_bookings.csv\") \n\n#Using required functions\nhotelBookings %>%\n  select(1:4) %>%\n  filter(arrival_date_year==2015) %>%\n  arrange(4)\n\n\n# A tibble: 21,996 × 4\n   hotel        is_canceled lead_time arrival_date_year\n   <chr>              <dbl>     <dbl>             <dbl>\n 1 Resort Hotel           0       342              2015\n 2 Resort Hotel           0       737              2015\n 3 Resort Hotel           0         7              2015\n 4 Resort Hotel           0        13              2015\n 5 Resort Hotel           0        14              2015\n 6 Resort Hotel           0        14              2015\n 7 Resort Hotel           0         0              2015\n 8 Resort Hotel           0         9              2015\n 9 Resort Hotel           1        85              2015\n10 Resort Hotel           1        75              2015\n# … with 21,986 more rows\n\nhotelBookings %>%\n  group_by(hotel) %>%\n  summarise(mean = mean(adr), n = n())\n\n\n# A tibble: 2 × 3\n  hotel         mean     n\n  <chr>        <dbl> <int>\n1 City Hotel   105.  79330\n2 Resort Hotel  95.0 40060\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:08-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-hw3nathaniel/",
    "title": "HW3Nathaniel",
    "description": "Reading in marriage data",
    "author": [],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:10-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-noahhw3/",
    "title": "NoahHw3",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\nChicken Data In the code below I am loading all of the packages I will be using throughout this assignment\n\n\nlibrary(tidyverse) \nlibrary(dplyr) \nlibrary(knitr) \nlibrary(readxl) \nlibrary(ggplot2) \nlibrary(broom) \n\n\n\nThe code below is my file path for importing the data regarding chicken meat prices in excel form\n\n\npoultry <- read_excel(\"../../_data/poultry_tidy.xlsx\") \n\n\n\nThe Poultry The Poultry data is broken into 4 columns, Product which is the cut of chicken, Year and Month corresponding to each observation. Price_Dollar represents the price of each cut at a month and year.\n\n\npoultry \n\n\n# A tibble: 600 × 4\n   Product  Year Month     Price_Dollar\n   <chr>   <dbl> <chr>            <dbl>\n 1 Whole    2013 January           2.38\n 2 Whole    2013 February          2.38\n 3 Whole    2013 March             2.38\n 4 Whole    2013 April             2.38\n 5 Whole    2013 May               2.38\n 6 Whole    2013 June              2.38\n 7 Whole    2013 July              2.38\n 8 Whole    2013 August            2.38\n 9 Whole    2013 September         2.38\n10 Whole    2013 October           2.38\n# … with 590 more rows\n\nBelow is my initial data table manipulation which I did to get a specific product, in this case boneless breast with its price and the year in which that price was observed\n\n\npoultry %>% group_by(`Price_Dollar`)%>% \n  \n# first line groups the data by the price in the \n#Price_Dollar column so the data in the column is sorted into chunks of the same price. \n  \nselect(!(`Month`))%>% \n  \n# This select function selects all columns except for the one called \"Month\" \n  \nfilter(Product==\"B/S Breast\") %>% \n  \n# The line above selects only the Product in the \n#\"Product\" column called \"B/S Breast, or boneless chicken breast\" \n  \narrange(desc(`Price_Dollar`)) %>% \n\n# The above line sorts or arranges the column of \n# Price in dollars or \"Price_Dollar\" in descending order \n#starting above 7 dollars and going down closer to 6 dollars \nrename(Chicken_Bonless_Breast_Price=Price_Dollar)\n\n\n# A tibble: 120 × 3\n# Groups:   Chicken_Bonless_Breast_Price [8]\n   Product     Year Chicken_Bonless_Breast_Price\n   <chr>      <dbl>                        <dbl>\n 1 B/S Breast  2013                         7.04\n 2 B/S Breast  2013                         7.04\n 3 B/S Breast  2013                         7.04\n 4 B/S Breast  2013                         7.04\n 5 B/S Breast  2013                         7.04\n 6 B/S Breast  2013                         7.04\n 7 B/S Breast  2013                         7.04\n 8 B/S Breast  2013                         7.04\n 9 B/S Breast  2013                         7.04\n10 B/S Breast  2013                         7.04\n# … with 110 more rows\n\n# This above line renames the column \n#\"Price_Dollar\" into column \"Chicken_Boneless_Breast_Price\" \n# The line above takes the poultry data frame, it then finds the mean price in dollars and removes all N/A observations \n\n\n\n\n\nsummarise(poultry, mean(`Price_Dollar`, na.rm = TRUE)) \n\n\n# A tibble: 1 × 1\n  `mean(Price_Dollar, na.rm = TRUE)`\n                               <dbl>\n1                               3.39\n\nThe first line groups the data by the price in the Price_Dollar column so the data in the column is sorted into chunks of the same price.\nThe second line selects all columns except for the one called “Month”\nThe third line selects only the Product in the “Product” column called “B/S Breast, or boneless chicken breast”\nThe fourth line sorts or arranges the column of Price in dollars or “Price_Dollar” in descending order starting above 7 dollars and going down closer to 6 dollars This fifth line renames the column “Price_Dollar” into column “Chicken_Boneless_Breast_Price”\nThe final line of codes above takes the poultry data frame, it then finds the mean price in dollars and removes all N/A observations\n\n\npoultry %>% group_by(Year, Price_Dollar, Product) %>% ggplot() + geom_smooth(mapping=aes(y=Price_Dollar, x=Year, color=Product), na.rm=TRUE) \n\n\n\n\nPoultry Plot Post and Conclusion\nBy Noah Milstein\nChicken Data Conclusion\nThe graph above suggests that the price of most chicken cuts remain relatively similar over time, however B/S Breast or boneless chicken breast appears to have increased in price over recent years. Thighs have also remained relatively similar\n\n\n\n",
    "preview": "posts/2021-08-17-noahhw3/noahhw3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-08-24T08:21:25-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-13-ishas-blog-post/",
    "title": "Isha's Blog Post",
    "description": "Running Blog Post for Course Homeworks",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": {}
      }
    ],
    "date": "2021-08-16",
    "categories": [],
    "contents": "\n##Introduction\nHello Everybody! My name is Isha and I am starting the DACSS program this summer. I am from India and completed my bachelors in journalism and political science from UMass. I am excited to meet everyone this fall and delve into the world of data and social science.\n##Homework 2\n\n\nmarriage_data <- read_csv('../../_data/australian_marriage_tidy.csv')\nhead(marriage_data)\n\n\n# A tibble: 6 × 4\n  territory       resp    count percent\n  <chr>           <chr>   <dbl>   <dbl>\n1 New South Wales yes   2374362    57.8\n2 New South Wales no    1736838    42.2\n3 Victoria        yes   2145629    64.9\n4 Victoria        no    1161098    35.1\n5 Queensland      yes   1487060    60.7\n6 Queensland      no     961015    39.3\n\n#About the Data This data sample is drawn from the Australian Marriage Law Postal Survey in 2017. Eligible participants were those who enrolled in the Commonwealth Electoral Roll by August 24 , 2017 and were 18 years of Age. They had not served prison sentences of three years and longer and participated in the survey voluntarily. The responses were recorded to the question\nShould the law be changed to allow same-sex couples to marry?\n##Homework 3\n\nHow can you arrange your dataset by territory?\n\n\n\ndata_summary <- marriage_data %>% \n  group_by(territory) %>% \n  summarise(count,resp)\n\n\n\n\nWhich Territory had the highest population count who voted yes to the law allowing same-sex couples to marry.\n\n\n\nyes_voters <-marriage_data %>% \n  filter(resp==\"yes\") %>% \n  arrange(-count) \n\n\n\n\nRename the resp column to response.\n\n\n\nmarriage_data <- marriage_data %>% \n  rename(response = resp)\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:23-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-hanae-homework-3/",
    "title": "Hanae- Homework 3",
    "description": "This is my submission of Homework 3.",
    "author": [
      {
        "name": "Hanae Bouazza",
        "url": {}
      }
    ],
    "date": "2021-08-16",
    "categories": [],
    "contents": "\n##Loading Library\n\n\nlibrary(tidyverse)\n\n\n\n##Data\n\n\nbookings<-read_csv(file=\"../../_data/hotel_bookings.csv\")\n\n\n\n##Using summarise() and group_by()\n\n\ngroup_by(bookings, hotel)%>%\nsummarise(meanAdr=mean(adr), n=n())\n\n\n# A tibble: 2 × 3\n  hotel        meanAdr     n\n  <chr>          <dbl> <int>\n1 City Hotel     105.  79330\n2 Resort Hotel    95.0 40060\n\n##Using arrange(), select(), and filter()\n\n\nstatusByCountry<-arrange(bookings, reservation_status)%>%\nselect(\"hotel\", \"country\", \"reservation_status\")\nfilter(statusByCountry, country==\"USA\")\n\n\n# A tibble: 2,097 × 3\n   hotel        country reservation_status\n   <chr>        <chr>   <chr>             \n 1 Resort Hotel USA     Canceled          \n 2 Resort Hotel USA     Canceled          \n 3 Resort Hotel USA     Canceled          \n 4 Resort Hotel USA     Canceled          \n 5 Resort Hotel USA     Canceled          \n 6 Resort Hotel USA     Canceled          \n 7 Resort Hotel USA     Canceled          \n 8 Resort Hotel USA     Canceled          \n 9 Resort Hotel USA     Canceled          \n10 Resort Hotel USA     Canceled          \n# … with 2,087 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-saulo-homework-two/",
    "title": "Saulo's Homeworks",
    "description": "Feeble attempts at data science.",
    "author": [
      {
        "name": "Saulo DePaula",
        "url": {}
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "saulo",
      "homeworks",
      "read data"
    ],
    "contents": "\nHomework Two\n\n\neggs <- read.csv(file=\"../../_data/eggs_tidy.csv\")\nhead(eggs)\n\n\n     month year large_half_dozen large_dozen extra_large_half_dozen\n1  January 2004            126.0     230.000                  132.0\n2 February 2004            128.5     226.250                  134.5\n3    March 2004            131.0     225.000                  137.0\n4    April 2004            131.0     225.000                  137.0\n5      May 2004            131.0     225.000                  137.0\n6     June 2004            133.5     231.375                  137.0\n  extra_large_dozen\n1             230.0\n2             230.0\n3             230.0\n4             234.5\n5             236.0\n6             241.0\n\nHomework Three\nSelect\n\n\nlibrary(tidyverse)\nselect(eggs, \"month\")\n\n\n        month\n1     January\n2    February\n3       March\n4       April\n5         May\n6        June\n7        July\n8      August\n9   September\n10    October\n11   November\n12   December\n13    January\n14   February\n15      March\n16      April\n17        May\n18       June\n19       July\n20     August\n21  September\n22    October\n23   November\n24   December\n25    January\n26   February\n27      March\n28      April\n29        May\n30       June\n31       July\n32     August\n33  September\n34    October\n35   November\n36   December\n37    January\n38   February\n39      March\n40      April\n41        May\n42       June\n43       July\n44     August\n45  September\n46    October\n47   November\n48   December\n49    January\n50   February\n51      March\n52      April\n53        May\n54       June\n55       July\n56     August\n57  September\n58    October\n59   November\n60   December\n61    January\n62   February\n63      March\n64      April\n65        May\n66       June\n67       July\n68     August\n69  September\n70    October\n71   November\n72   December\n73    January\n74   February\n75      March\n76      April\n77        May\n78       June\n79       July\n80     August\n81  September\n82    October\n83   November\n84   December\n85    January\n86   February\n87      March\n88      April\n89        May\n90       June\n91       July\n92     August\n93  September\n94    October\n95   November\n96   December\n97    January\n98   February\n99      March\n100     April\n101       May\n102      June\n103      July\n104    August\n105 September\n106   October\n107  November\n108  December\n109   January\n110  February\n111     March\n112     April\n113       May\n114      June\n115      July\n116    August\n117 September\n118   October\n119  November\n120  December\n\nFilter\n\n\nlibrary(tidyverse)\nfilter(eggs, `month` == \"January\")\n\n\n     month year large_half_dozen large_dozen extra_large_half_dozen\n1  January 2004            126.0       230.0                 132.00\n2  January 2005            128.5       233.5                 135.50\n3  January 2006            128.5       233.5                 135.50\n4  January 2007            128.5       233.5                 135.50\n5  January 2008            132.0       237.0                 139.00\n6  January 2009            174.5       277.5                 185.50\n7  January 2010            174.5       271.5                 185.50\n8  January 2011            174.5       267.5                 185.50\n9  January 2012            174.5       267.5                 185.50\n10 January 2013            178.0       267.5                 188.13\n   extra_large_dozen\n1              230.0\n2              241.0\n3              241.0\n4              241.5\n5              245.0\n6              285.5\n7              285.5\n8              285.5\n9              285.5\n10             290.0\n\nArrange\n\n\nlibrary(tidyverse)\nfilter(eggs, `large_half_dozen` > 130) %>%\n  arrange(`large_half_dozen`)\n\n\n       month year large_half_dozen large_dozen extra_large_half_dozen\n1      March 2004          131.000     225.000                137.000\n2      April 2004          131.000     225.000                137.000\n3        May 2004          131.000     225.000                137.000\n4   February 2007          131.125     236.125                138.125\n5      March 2007          132.000     237.000                139.000\n6      April 2007          132.000     237.000                139.000\n7        May 2007          132.000     237.000                139.000\n8       June 2007          132.000     237.000                139.000\n9       July 2007          132.000     237.000                139.000\n10    August 2007          132.000     237.000                139.000\n11 September 2007          132.000     237.000                139.000\n12   October 2007          132.000     237.000                139.000\n13  November 2007          132.000     237.000                139.000\n14  December 2007          132.000     237.000                139.000\n15   January 2008          132.000     237.000                139.000\n16  February 2008          132.000     237.000                139.000\n17     March 2008          132.000     237.000                139.000\n18     April 2008          132.000     237.000                139.000\n19       May 2008          132.000     237.000                139.000\n20      June 2004          133.500     231.375                137.000\n21      July 2004          133.500     233.500                137.000\n22    August 2004          133.500     233.500                137.000\n23       May 2012          173.250     267.500                185.500\n24      June 2012          173.250     267.500                185.500\n25      July 2012          173.250     267.500                185.500\n26    August 2012          173.250     267.500                185.500\n27 September 2012          173.250     267.500                185.500\n28   October 2012          173.250     267.500                185.500\n29      June 2008          174.500     277.500                185.500\n30      July 2008          174.500     277.500                185.500\n31    August 2008          174.500     277.500                185.500\n32 September 2008          174.500     277.500                185.500\n33   October 2008          174.500     277.500                185.500\n34  November 2008          174.500     277.500                185.500\n35  December 2008          174.500     277.500                185.500\n36   January 2009          174.500     277.500                185.500\n37  February 2009          174.500     277.500                185.500\n38     March 2009          174.500     277.500                185.500\n39     April 2009          174.500     277.500                185.500\n40       May 2009          174.500     277.500                185.500\n41      June 2009          174.500     277.500                185.500\n42      July 2009          174.500     277.500                185.500\n43    August 2009          174.500     271.500                185.500\n44 September 2009          174.500     271.500                185.500\n45   October 2009          174.500     271.500                185.500\n46  November 2009          174.500     271.500                185.500\n47  December 2009          174.500     271.500                185.500\n48   January 2010          174.500     271.500                185.500\n49  February 2010          174.500     271.500                185.500\n50     March 2010          174.500     268.000                185.500\n51     April 2010          174.500     268.000                185.500\n52       May 2010          174.500     268.000                185.500\n53      June 2010          174.500     268.000                185.500\n54      July 2010          174.500     268.000                185.500\n55    August 2010          174.500     268.000                185.500\n56 September 2010          174.500     268.000                185.500\n57   October 2010          174.500     267.500                185.500\n58  November 2010          174.500     267.500                185.500\n59  December 2010          174.500     267.500                185.500\n60   January 2011          174.500     267.500                185.500\n61  February 2011          174.500     267.500                185.500\n62     March 2011          174.500     267.500                185.500\n63     April 2011          174.500     267.500                185.500\n64       May 2011          174.500     267.500                185.500\n65      June 2011          174.500     270.000                185.500\n66      July 2011          174.500     270.000                185.500\n67    August 2011          174.500     270.000                185.500\n68 September 2011          174.500     270.000                185.500\n69   October 2011          174.500     270.000                185.500\n70  November 2011          174.500     270.000                185.500\n71  December 2011          174.500     270.000                185.500\n72   January 2012          174.500     267.500                185.500\n73  February 2012          174.500     267.500                185.500\n74     March 2012          174.500     267.500                185.500\n75     April 2012          174.500     267.500                185.500\n76  November 2012          178.000     267.500                188.130\n77  December 2012          178.000     267.500                188.130\n78   January 2013          178.000     267.500                188.130\n79  February 2013          178.000     267.500                188.130\n80     March 2013          178.000     267.500                188.130\n81     April 2013          178.000     267.500                188.130\n82       May 2013          178.000     267.500                188.130\n83      June 2013          178.000     267.500                188.130\n84      July 2013          178.000     267.500                188.130\n85    August 2013          178.000     267.500                188.130\n86 September 2013          178.000     267.500                188.130\n87   October 2013          178.000     267.500                188.130\n88  November 2013          178.000     267.500                188.130\n89  December 2013          178.000     267.500                188.130\n   extra_large_dozen\n1            230.000\n2            234.500\n3            236.000\n4            244.125\n5            245.000\n6            245.000\n7            245.000\n8            245.000\n9            245.000\n10           245.000\n11           245.000\n12           245.000\n13           245.000\n14           245.000\n15           245.000\n16           245.000\n17           245.000\n18           245.000\n19           245.000\n20           241.000\n21           241.000\n22           241.000\n23           288.500\n24           288.500\n25           288.500\n26           288.500\n27           288.500\n28           288.500\n29           285.500\n30           285.500\n31           285.500\n32           285.500\n33           285.500\n34           285.500\n35           285.500\n36           285.500\n37           285.500\n38           285.500\n39           285.500\n40           285.500\n41           285.500\n42           285.500\n43           285.500\n44           285.500\n45           285.500\n46           285.500\n47           285.500\n48           285.500\n49           285.500\n50           285.500\n51           285.500\n52           285.500\n53           285.500\n54           285.500\n55           285.500\n56           285.500\n57           285.500\n58           285.500\n59           285.500\n60           285.500\n61           285.500\n62           285.500\n63           285.500\n64           285.500\n65           285.500\n66           285.500\n67           285.500\n68           285.500\n69           285.500\n70           285.500\n71           285.500\n72           285.500\n73           288.500\n74           288.500\n75           288.500\n76           290.000\n77           290.000\n78           290.000\n79           290.000\n80           290.000\n81           290.000\n82           290.000\n83           290.000\n84           290.000\n85           290.000\n86           290.000\n87           290.000\n88           290.000\n89           290.000\n\nSummarize\n\n\nlibrary(tidyverse)\nsummarize(eggs, mean(`large_half_dozen`))\n\n\n  mean(large_half_dozen)\n1               155.1656\n\nHomework Four\nEgg Data for Large Half Dozens\nThe following is a brief set of summary statistics from the eggs_tidy dataset, specifically looking at the price (in cents) per Large Half Dozen Eggs from January 2004 to December 2013. I have provided the mean, median, min, and max for this variable, along with a basic visualization. No data cleaning or recoding was necessary, given the provided dataset was sufficiently clean.\n\n\nlibrary(tidyverse)\nsummarize(eggs, mean(`large_half_dozen`))\n\n\n  mean(large_half_dozen)\n1               155.1656\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, median(`large_half_dozen`))\n\n\n  median(large_half_dozen)\n1                    174.5\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, min(`large_half_dozen`))\n\n\n  min(large_half_dozen)\n1                   126\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, max(`large_half_dozen`))\n\n\n  max(large_half_dozen)\n1                   178\n\n\n\nlibrary(tidyverse)\nggplot(eggs, aes(`large_half_dozen`)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"Large Half Dozen Egg Prices (in cents) | Jan. 2004 to Dec. 2013\", y = \"Count of Occurances\", x= \"Count of Eggs\")\n\n\n\n\nEgg Data for Large Dozens\nThe following is a brief set of summary statistics from the eggs_tidy dataset, specifically looking at the price (in cents) per Large Dozen Eggs from January 2004 to December 2013. I have provided the mean, median, min, and max for this variable, along with a basic visualization. No data cleaning or recoding was necessary, given the provided dataset was sufficiently clean.\n\n\nlibrary(tidyverse)\nsummarize(eggs, mean(`large_dozen`))\n\n\n  mean(large_dozen)\n1          254.1979\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, median(`large_dozen`))\n\n\n  median(large_dozen)\n1               267.5\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, min(`large_dozen`))\n\n\n  min(large_dozen)\n1              225\n\n\n\nlibrary(tidyverse)\nsummarize(eggs, max(`large_dozen`))\n\n\n  max(large_dozen)\n1            277.5\n\n\n\nlibrary(tidyverse)\nggplot(eggs, aes(`large_dozen`)) + geom_histogram() +\n  theme_minimal() +\n  labs(title = \"Large Dozen Eggs (in cents) | Jan. 2004 to Dec. 2013\", y = \"Count of Occurances\", x= \"Count of Eggs\")\n\n\n\n\nHomework Five\n\n\nlibrary(tidyverse)\nggplot(eggs, aes(x=`year`, y=`large_half_dozen`, col=as_factor(`extra_large_half_dozen`))) + \n  geom_point()\n\n\n\n\n\n\nlibrary(tidyverse)\nggplot(eggs, aes(x=`year`, y=`large_dozen`, col=as_factor(`extra_large_dozen`))) + \n  geom_point()\n\n\n\n\nWhat these visualizations represent: There is an equal relationship between the prices of Large Half Dozen and Extra Large Half Dozen eggs, as well as between the prices of Large Dozen and Extra Large Dozen eggs. Essentially, it appears they go up at the same rate over time; if the Large variation goes up, so too does the Extra Large variation.\nWhy I chose this visualization approach, and what alternative approaches I considered but decided not to pursue: These visualizations are very straightforward and clearly demonstrate the relationships between the Large and Extra Large egg variations. I attempted a histogram, but received a very lengthy error, which led me to feel content with the geom_point() option.\nWhat I wished, if anything, I could have executed but found limited capability to do: It would have been nice to produce a larger visual that included month and year, to spread the points out even further, but I was unsure of how to do that. I also wish there were more data to utilize, such as the price of chickens, which would be interesting to compare to the price of eggs.\n\n\n\n",
    "preview": "posts/2021-08-16-saulo-homework-two/saulo-homework-two_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-08-24T08:20:48-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-15-zoe-hotel-bookings-dataset/",
    "title": "Zoe Hotel Bookings Dataset",
    "description": "An introdution to the hotel dataset",
    "author": [
      {
        "name": "Zoe Bean",
        "url": {}
      }
    ],
    "date": "2021-08-15",
    "categories": [],
    "contents": "\nHomework 2\nI will be processing a dataset about hotel bookings from 2015 to 2017. First, I import the dataset, which requires the tidyverse package to be loaded.\n\n\nlibrary(tidyverse)\nhotel_data=read_csv(\"../../_data/hotel_bookings.csv\")\n\n\n\nNext, I use head() to give an example of what the dataset looks like.\n\n\nhead(hotel_data)\n\n\n# A tibble: 6 × 32\n  hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n  <chr>              <dbl>     <dbl>            <dbl> <chr>           \n1 Resort Hotel           0       342             2015 July            \n2 Resort Hotel           0       737             2015 July            \n3 Resort Hotel           0         7             2015 July            \n4 Resort Hotel           0        13             2015 July            \n5 Resort Hotel           0        14             2015 July            \n6 Resort Hotel           0        14             2015 July            \n# … with 27 more variables: arrival_date_week_number <dbl>,\n#   arrival_date_day_of_month <dbl>, stays_in_weekend_nights <dbl>,\n#   stays_in_week_nights <dbl>, adults <dbl>, children <dbl>,\n#   babies <dbl>, meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nTo find out how many rows are in the dataset, I use dim(). I also use colnames() to figure out what the data drawn from each observation are.\n\n\ndim(hotel_data)\n\n\n[1] 119390     32\n\ncolnames(hotel_data)\n\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n\nThe result from dim() means that there are 119390 rows and 32 columns. This is important since the number of rows tells us how many hotel bookings there are in this dataset, and the columns tell us how many pieces of data are available per booking.\nHomework 3\nThe colnames function is helpful for this next step, where I select columns. Here, I select the year of arrival and the month.\n\n\nselect(hotel_data, arrival_date_year, arrival_date_month )\n\n\n# A tibble: 119,390 × 2\n   arrival_date_year arrival_date_month\n               <dbl> <chr>             \n 1              2015 July              \n 2              2015 July              \n 3              2015 July              \n 4              2015 July              \n 5              2015 July              \n 6              2015 July              \n 7              2015 July              \n 8              2015 July              \n 9              2015 July              \n10              2015 July              \n# … with 119,380 more rows\n\nI can do more with select, such as selecting all columns that start with ‘arrival_date’ to get more clear information about when each the booking is.\n\n\nselect(hotel_data, starts_with(\"arrival_date\"))\n\n\n# A tibble: 119,390 × 4\n   arrival_date_ye… arrival_date_mo… arrival_date_we… arrival_date_da…\n              <dbl> <chr>                       <dbl>            <dbl>\n 1             2015 July                           27                1\n 2             2015 July                           27                1\n 3             2015 July                           27                1\n 4             2015 July                           27                1\n 5             2015 July                           27                1\n 6             2015 July                           27                1\n 7             2015 July                           27                1\n 8             2015 July                           27                1\n 9             2015 July                           27                1\n10             2015 July                           27                1\n# … with 119,380 more rows\n\nIf I want to look at all the bookings where there are no children, I use filter() as follows:\n\n\nfilter(hotel_data, children== 0)\n\n\n# A tibble: 110,796 × 32\n   hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n   <chr>              <dbl>     <dbl>            <dbl> <chr>           \n 1 Resort Hotel           0       342             2015 July            \n 2 Resort Hotel           0       737             2015 July            \n 3 Resort Hotel           0         7             2015 July            \n 4 Resort Hotel           0        13             2015 July            \n 5 Resort Hotel           0        14             2015 July            \n 6 Resort Hotel           0        14             2015 July            \n 7 Resort Hotel           0         0             2015 July            \n 8 Resort Hotel           0         9             2015 July            \n 9 Resort Hotel           1        85             2015 July            \n10 Resort Hotel           1        75             2015 July            \n# … with 110,786 more rows, and 27 more variables:\n#   arrival_date_week_number <dbl>, arrival_date_day_of_month <dbl>,\n#   stays_in_weekend_nights <dbl>, stays_in_week_nights <dbl>,\n#   adults <dbl>, children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nIf I want to arrange the hotel data by putting a column in order, I use arrange(). In this example, I order the data by the arrival month, which is sorted alphabetically.\n\n\narrange(hotel_data, arrival_date_month)\n\n\n# A tibble: 119,390 × 32\n   hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n   <chr>              <dbl>     <dbl>            <dbl> <chr>           \n 1 Resort Hotel           1        31             2016 April           \n 2 Resort Hotel           0         0             2016 April           \n 3 Resort Hotel           0       144             2016 April           \n 4 Resort Hotel           0       144             2016 April           \n 5 Resort Hotel           0       144             2016 April           \n 6 Resort Hotel           0       163             2016 April           \n 7 Resort Hotel           1        38             2016 April           \n 8 Resort Hotel           0       175             2016 April           \n 9 Resort Hotel           1        39             2016 April           \n10 Resort Hotel           1        32             2016 April           \n# … with 119,380 more rows, and 27 more variables:\n#   arrival_date_week_number <dbl>, arrival_date_day_of_month <dbl>,\n#   stays_in_weekend_nights <dbl>, stays_in_week_nights <dbl>,\n#   adults <dbl>, children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nIf I actually wanted to have the months in order, I would have to make a numerical variable in the dataset using mutate() and case_when() and then arrange by that variable.\n\n\nhotel_data<- hotel_data %>%\n  mutate(arrival_date_month_num = case_when(\n         arrival_date_month == \"January\" ~ 1,\n         arrival_date_month == \"February\" ~ 2,\n         arrival_date_month == \"March\" ~ 3,\n         arrival_date_month == \"April\" ~ 4,\n         arrival_date_month == \"May\" ~ 5,\n         arrival_date_month == \"June\" ~ 6,\n         arrival_date_month == \"July\" ~ 7,\n         arrival_date_month == \"August\" ~ 8,\n         arrival_date_month == \"September\" ~ 9,\n         arrival_date_month == \"October\" ~ 10,\n         arrival_date_month == \"November\" ~ 11,\n         arrival_date_month == \"December\" ~ 12\n         ))\n\narrange(hotel_data, arrival_date_month_num)\n\n\n# A tibble: 119,390 × 33\n   hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n   <chr>              <dbl>     <dbl>            <dbl> <chr>           \n 1 Resort Hotel           0       109             2016 January         \n 2 Resort Hotel           0       109             2016 January         \n 3 Resort Hotel           1         2             2016 January         \n 4 Resort Hotel           0        88             2016 January         \n 5 Resort Hotel           1        20             2016 January         \n 6 Resort Hotel           1        76             2016 January         \n 7 Resort Hotel           0        88             2016 January         \n 8 Resort Hotel           1       113             2016 January         \n 9 Resort Hotel           1       113             2016 January         \n10 Resort Hotel           1       113             2016 January         \n# … with 119,380 more rows, and 28 more variables:\n#   arrival_date_week_number <dbl>, arrival_date_day_of_month <dbl>,\n#   stays_in_weekend_nights <dbl>, stays_in_week_nights <dbl>,\n#   adults <dbl>, children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nIf I wanted to know the average amount of adults per booking, I would use summarise() like so:\n\n\nsummarise(hotel_data, mean=mean(adults))\n\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1  1.86\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:30-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-19-nathanielreadindata/",
    "title": "Australian Marriage",
    "description": "Statistics About Australian Marriage",
    "author": [
      {
        "name": "Nathaniel Ross",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [],
    "contents": "\n\n# A tibble: 16 × 4\n   territory                       resp    count percent\n   <chr>                           <chr>   <dbl>   <dbl>\n 1 New South Wales                 yes   2374362    57.8\n 2 New South Wales                 no    1736838    42.2\n 3 Victoria                        yes   2145629    64.9\n 4 Victoria                        no    1161098    35.1\n 5 Queensland                      yes   1487060    60.7\n 6 Queensland                      no     961015    39.3\n 7 South Australia                 yes    592528    62.5\n 8 South Australia                 no     356247    37.5\n 9 Western Australia               yes    801575    63.7\n10 Western Australia               no     455924    36.3\n11 Tasmania                        yes    191948    63.6\n12 Tasmania                        no     109655    36.4\n13 Northern Territory(b)           yes     48686    60.6\n14 Northern Territory(b)           no      31690    39.4\n15 Australian Capital Territory(c) yes    175459    74  \n16 Australian Capital Territory(c) no      61520    26  \n\nRead in data on Australian Marriage\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:22:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-random/",
    "title": "Leah's first post: Attempt 4",
    "description": "My first blog post.",
    "author": [
      {
        "name": "Leah Dion",
        "url": {}
      }
    ],
    "date": "2021-08-12",
    "categories": [
      ".ruminating .random"
    ],
    "contents": "\nEducation/Work Background: I worked in retail management for many years and am currently entering my final semester to finish my undergraduate degree in Math/Stats.\nProgram: DACSS\nR experience: I used R for several classes and did several projects in the past few years.\nResearch interests: Public Policy, machine learning, criminal punishment system, and social movements\nHometown: South Hadley, MA\nHobbies: Music, basketball, spending time with my cat Jordan.\nFun fact: I have bowled a perfect game (300).\nBelow is a scatter plot of 250 randomly generated data points.\n\n\nknitr::opts_chunk$set(echo = TRUE)\n#set seed to reproduce results\nset.seed(327)\n#randomly generate 250 int from 1-100\nX <- sample.int(100, 250, replace=TRUE) \n#randomly generate 250 int from 1-100\nY <- sample.int(100, 250, replace=TRUE)\n#scatter plot of X, Y\nplot(X, Y, xlab=\"X Samples\", ylab=\"Y Samples\",\n     pch = 16, col = \"red\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-10-random/leahs-first-post-attempt-4_files/figure-html5/setup-1.png",
    "last_modified": "2021-08-23T22:51:03-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-12-alligator-food/",
    "title": "Alligator Food",
    "description": "The diet of Alligators throughout several different lakes. Data has the lake, sex of alligator, size of food, and species of animal used for food.",
    "author": [],
    "date": "2021-08-12",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:57-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-12-leahs-first-post-attempt-2/",
    "title": "Leah's first post: Attempt 2",
    "description": "My first blog post.",
    "author": [
      {
        "name": "-Leah Dion",
        "url": {}
      }
    ],
    "date": "2021-08-12",
    "categories": [
      "-ruminating -random"
    ],
    "contents": "\nEducation/Work Background: I worked in retail management for many years and am currently entering my final semester to finish my undergraduate degree in Math/Stats.\nProgram: DACSS\nR experience: I used R for several classes and did several projects in the past few years.\nResearch interests: Public Policy, machine learning, criminal punishment system, and social movements\nHometown: South Hadley, MA\nHobbies: Music, basketball, spending time with my cat Jordan.\nFun fact: I have bowled a perfect game (300).\nBelow is a scatter plot of 250 randomly generated data points.\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nset.seed(327)\nX <- sample.int(100, 250, replace=TRUE)\nY <- sample.int(100, 250, replace=TRUE)\nplot(X, Y, xlab=\"X Samples\", ylab=\"Y Samples\",\n     pch = 16, col = \"red\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-12-leahs-first-post-attempt-2/leahs-first-post-attempt-2_files/figure-html5/setup-1.png",
    "last_modified": "2021-08-24T08:20:00-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-12-organic-egg-poultry-filtered_hw2_bakahria/",
    "title": "Organic Egg Poultry Filtered",
    "description": "This post consists of all the plots obtained from the filtered data vs Index(Month-Year)",
    "author": [],
    "date": "2021-08-12",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nPlots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-12-organic-egg-poultry-filtered_hw2_bakahria/organic-egg-poultry-filtered_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-08-24T08:20:18-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-12-wrangling-the-australian-marriage-law-dataset/",
    "title": "Wrangling the Australian marriage law dataset",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Karl Tacheron",
        "url": {}
      }
    ],
    "date": "2021-08-12",
    "categories": [],
    "contents": "\nThe Australian Bureau of Statistics 2017 Marriage Law Postal survey contains data about a nationwide vote that took place by mail. The data has a few things making reading into a tibble difficult:\nGrouped information instead of individual observations where variables appear elsewhere\nMany extraneous & repeated calculated variables\nMulti-index data formatted visually into blocks\nMulti-index column names with confusing and unhelpful names\nTo make this data usable we must transform its structure in both its column layout and its rows.\nWe read in the Excel file’s third sheet, cut it down to only the needed variables and rows, rename the columns, and remove NA values. We also remove rows that contain section totals.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nvotes <- read_excel(\"../../_data/australian_marriage_law_postal_survey_2017_-_response_final.xls\",\n           sheet=\"Table 2\",\n           skip=7,\n           col_names = c(\"Town\", \"Yes\", \"d\", \"No\", rep(\"d\", 6), \"Illegible\", \"d\", \"No Response\", rep(\"d\", 3)))%>%\n  select(!starts_with(\"d\"))%>%\n  drop_na(Town)%>%\n  filter(!str_detect(Town, \"(Total)\"))%>%\n  filter(!str_starts(Town, \"\\\\(\"))\n\n\n\nThe last step is more complicated. Each observation needs a variable for is administrative “division”, but this is displayed at the top of each block. These junk rows listing the parent division names must be turned into a variable for each row.\nWe get the number of each row that contains \" Divisions\".\n\n\nvotes<- votes%>%\n  mutate(Divisions = case_when(\n    str_ends(Town, \"Divisions\") ~ Town,\n    TRUE ~ NA_character_\n  ))\n\nfor(i in 1:length(votes$Divisions)){\n  votes$Divisions[i]<-ifelse(is.na(votes$Divisions[i]),votes$Divisions[i-1], votes$Divisions[i])\n}\n\nvotes<- filter(votes,!str_detect(Town, \"Divisions|Australia\"))\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:20:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-2012-us-railroad-employment/",
    "title": "2012 US Railroad Employment",
    "description": "This is Shih-Yen's post on importing and tidying data",
    "author": [],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nHello, in this post, I will introduce the 2012 US Railroad Employment data, discuss some of the issues with the data, and provide the R code I used to import and tidy the data.\nFirst, let’s download the data. Since the data is in a .xls file, I used the readxl library that comes with the tidyverse package to import the data.\nlibrary(readxl)\nrailroad_data <- read_excel(\"StateCounty2012.xls\")\nview(railroad_data)\nWhen viewing the data, you might first see that the column names for the variables are incorrect and the correct column names are actually in the third row. This error occurs because there are 3 lines of metadata at the top of the file, and read_excel uses the first line as the column names. To fix this problem, we can use skip = 3 in read_excel to skip the first 3 lines.\nrailroad_data <- read_excel(\"StateCounty2012.xls\", skip = 3)\nNext, you might also see that there are two columns, column 2 and 4, that have nothing but NA as values. Here’s an easy way to get rid of those columns:\nrailroad_data <- railroad_data[,-c(2, 4)]\nFinally, it is likely that we are only interested in U.S. county-level data, but our data file also contains rows for state totals, a row for Canada, and a row for the grand total employment in U.S railroads. In addition, there are notes and footnotes that are not useful for the purpose of data analysis.\nTo get of these rows, I use the fact that all of these rows contain entries that have NA as a value, and the rows we want to keep do not. Hence, to clean our data of these rows, we simply get rid of any rows that contain NA as a value. We can achieve that with the following line of code that subsets the railroad_data by omitting all the rows containing NA:\nrailroad_data_clean <- na.omit(railroad_data)\nNow, we have a county-level data set with clearly defined column/variable names.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:33-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-blog-1/",
    "title": "Larri Miller: Intro to [ ] Dataset",
    "description": "Blog Post 1",
    "author": [
      {
        "name": "Larri Miller",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [
      "Dataset type"
    ],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:35-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-first-post/",
    "title": "[First Post]",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Abhijit",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:40-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-hw1/",
    "title": "HW1",
    "description": "A short description of the post.",
    "author": [],
    "date": "2021-08-11",
    "categories": [
      "homework 1",
      "Antonis Gounalakis"
    ],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = FALSE)\n\nvac_tn <- c(52.01,39.84,33.42,5.8,7.29,4.73,4.29,0.55)\npop <- c(83.78,60.46,46.75,10.42,10.2,8.65,5.79,0.89)\nvac_sh <- vac_tn/pop\nperc_vac_sh <- vac_sh*100\nperc_vac_sh > 70\n\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nAssuming that each vaccine dose provides full immunity and therefore is administered to different individuals, I examined the vaccination rates of 8 countries (Germany, Italy, Spain, Greece, Portugal, Switzerland, Denmark, Cypurs). I found that only in 3 cases (Spain, Portugal,Denmark) the herd immunity threshold of 70% has been reached!\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:42-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-iris/",
    "title": "Arbitrary",
    "description": "My life in DACSS and with MAX :)",
    "author": [
      {
        "name": "Abhinav Kumar",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\n\n\n# Analysis of Iris\n\nlibrary(datasets)\n\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nplot(iris)\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-11-iris/Abhinav_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-24T08:19:45-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-11-larri-miller-exploring-australian-marriage/",
    "title": "Larri Miller - Exploring Australian Marriage",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Larri Miller",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nHere I am reading in my CSV file.\n\n        territory resp   count percent\n1 New South Wales  yes 2374362    57.8\n2 New South Wales   no 1736838    42.2\n3        Victoria  yes 2145629    64.9\n4        Victoria   no 1161098    35.1\n5      Queensland  yes 1487060    60.7\n6      Queensland   no  961015    39.3\n\nThis dataset explores Australian marriage. It has 16 observations of 4 variables.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:47-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-loading-data/",
    "title": "Loading a data set",
    "description": "Homework assignment to load data in to a R Markdown file.",
    "author": [
      {
        "name": "Karl Tacheron",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nLoad the tidyverse library:\n\n\nlibrary(tidyverse)\n\n\n\nImport a sample data set from the working directory and assign it to variable poultry:\n\n\npoultry<-read_csv('data/poultry_tidy.csv') \n\n\n\nFixed direction of the assignment - Meredith\nShow the first few values:\n\n\nhead(poultry)\n\n\n# A tibble: 6 × 4\n  Product  Year Month    Price_Dollar\n  <chr>   <dbl> <chr>           <dbl>\n1 Whole    2013 January          2.38\n2 Whole    2013 February         2.38\n3 Whole    2013 March            2.38\n4 Whole    2013 April            2.38\n5 Whole    2013 May              2.38\n6 Whole    2013 June             2.38\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:49-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-noahdata/",
    "title": "Noah_Chicken_Data",
    "description": "Included is a brief analysis of chicken data with a plot between year and the average price of each cut of chicken.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": "http://umass.edu/sbs/dacss"
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\n\n\n\n\n\npoultry <- read_csv(\"../../_data/eggs_tidy.csv\")\n\n\n\n\n\npoultry \n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen\n   <chr>     <dbl>            <dbl>       <dbl>                  <dbl>\n 1 January    2004             126         230                    132 \n 2 February   2004             128.        226.                   134.\n 3 March      2004             131         225                    137 \n 4 April      2004             131         225                    137 \n 5 May        2004             131         225                    137 \n 6 June       2004             134.        231.                   137 \n 7 July       2004             134.        234.                   137 \n 8 August     2004             134.        234.                   137 \n 9 September  2004             130.        234.                   136.\n10 October    2004             128.        234.                   136.\n# … with 110 more rows, and 1 more variable: extra_large_dozen <dbl>\n\nNote: This code isn’t running because the variables you are using aren’t in the original csv that I found - but this might be because you are using the poultry file not the eggs file. Sorry I couldn’t get it to work - Meredith\n\n\n#poultry %>% group_by(year, Price_Dollar, Product) %>% ggplot() +\n#  geom_line(mapping=aes(y=Price_Dollar, x=Year, color=Product), na.rm=TRUE)\n\n\n\nChicken Data Conclusion\nThe graph above suggests that the price of most chicken cuts remain relatively similar over time, however B/S Breast or boneless chicken breast appears to have increased in price over recent years. Thighs have also remained relatively similar\nNoah Milstein\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-post-1-hmwk-1/",
    "title": "Post 1 HMWK 1",
    "description": "This is my post for the first homework",
    "author": [
      {
        "name": "Annie McGrew",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [
      "homework 1",
      "Annie McGrew"
    ],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = FALSE)\nvector <- c(1,2,3,4,5)\nnew_vector <- c(3,5,1,1,2)\navg_vector <- (vector + new_vector)/2\nperc_vector <- avg_vector/5\nfinal_vector <- perc_vector*100\n\n\n\nFirst I input two vectors: vector = 1, 2, 3, 4, 5 and new_vector = 3, 5, 1, 1, 2 Then I take the average of these two vectors creating avg_vector = 2, 3.5, 2, 2.5, 3.5 Then I divide the average of the vectors by 5 (perc_vector = 0.4, 0.7, 0.4, 0.5, 0.7) and finally I multiple the vector by 100 (final_vector = 40, 70, 40, 50, 70).\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-saulo-homework-one/",
    "title": "Saulo Homework One",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Saulo DePaula",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-jason-wierzbowski-exploring-eggs/",
    "title": "Jason Wierzbowski - Exploring Eggs",
    "description": "A data set about eggs.",
    "author": [
      {
        "name": "Jason Wierzbowski",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nReading in my CSV file\n\n\n\nHomework #3\n\n# A tibble: 6 × 6\n  month     year large_half_dozen large_dozen extra_large_half_dozen\n  <chr>    <dbl>            <dbl>       <dbl>                  <dbl>\n1 January   2004             126         230                    132 \n2 February  2004             128.        226.                   134.\n3 March     2004             131         225                    137 \n4 April     2004             131         225                    137 \n5 May       2004             131         225                    137 \n6 June      2004             134.        231.                   137 \n# … with 1 more variable: extra_large_dozen <dbl>\n# A tibble: 1 × 2\n  `mean(extra_large_dozen)` `mean(extra_large_half_dozen)`\n                      <dbl>                          <dbl>\n1                      287.                           186.\n\nWorking on Regression of if certain months have an impact on the number of eggs produced\n\n# A tibble: 20 × 6\n# Groups:   month [2]\n   month    year large_half_dozen large_dozen extra_large_half_dozen\n   <chr>   <dbl>            <dbl>       <dbl>                  <dbl>\n 1 January  2004             126         230                    132 \n 2 July     2004             134.        234.                   137 \n 3 January  2005             128.        234.                   136.\n 4 July     2005             128.        234.                   136.\n 5 January  2006             128.        234.                   136.\n 6 July     2006             128.        234.                   136.\n 7 January  2007             128.        234.                   136.\n 8 July     2007             132         237                    139 \n 9 January  2008             132         237                    139 \n10 July     2008             174.        278.                   186.\n11 January  2009             174.        278.                   186.\n12 July     2009             174.        278.                   186.\n13 January  2010             174.        272.                   186.\n14 July     2010             174.        268                    186.\n15 January  2011             174.        268.                   186.\n16 July     2011             174.        270                    186.\n17 January  2012             174.        268.                   186.\n18 July     2012             173.        268.                   186.\n19 January  2013             178         268.                   188.\n20 July     2013             178         268.                   188.\n# … with 1 more variable: extra_large_dozen <dbl>\n\nThis data set contains information on how many half dozens and dozens of eggs made to be sold in a given month of a given year.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:21:22-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-18-rhyslongeggs/",
    "title": "Rhys_Long_Eggs",
    "description": "This is a dataset from a company that sells eggs. The data in this dataset provides a record of the company's monthly earnigns from January 2004 to December 2013.",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\nBelow is the uncleaned dataset. Even though this dataset has six columns, there are only three variables. The variables are the date when the data was collected, the size of the eggs that were sold, and the quantity of eggs sold per carton.\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\neggs_data <- read.csv(file = \"../../_data/eggs_tidy.csv\")\ntibble(eggs_data)\n\n\n# A tibble: 120 × 6\n   month      year large_half_dozen large_dozen extra_large_half_dozen\n   <chr>     <int>            <dbl>       <dbl>                  <dbl>\n 1 January    2004             126         230                    132 \n 2 February   2004             128.        226.                   134.\n 3 March      2004             131         225                    137 \n 4 April      2004             131         225                    137 \n 5 May        2004             131         225                    137 \n 6 June       2004             134.        231.                   137 \n 7 July       2004             134.        234.                   137 \n 8 August     2004             134.        234.                   137 \n 9 September  2004             130.        234.                   136.\n10 October    2004             128.        234.                   136.\n# … with 110 more rows, and 1 more variable: extra_large_dozen <dbl>\n\nIn order to show the accurate amount of columns in my data, I first used the unify() function to put month and year into the same column that I labeled as “Date”.\n\n\nlibrary(tidyverse)\nunite(eggs_data, \"Date\", month:year) %>%\n  as.tibble(eggs_data)\n\n\n# A tibble: 120 × 5\n   Date           large_half_dozen large_dozen extra_large_half_dozen\n   <chr>                     <dbl>       <dbl>                  <dbl>\n 1 January_2004               126         230                    132 \n 2 February_2004              128.        226.                   134.\n 3 March_2004                 131         225                    137 \n 4 April_2004                 131         225                    137 \n 5 May_2004                   131         225                    137 \n 6 June_2004                  134.        231.                   137 \n 7 July_2004                  134.        234.                   137 \n 8 August_2004                134.        234.                   137 \n 9 September_2004             130.        234.                   136.\n10 October_2004               128.        234.                   136.\n# … with 110 more rows, and 1 more variable: extra_large_dozen <dbl>\n\nNext, I used the pivot_longer function in order to condense the large_half_dozen, large_dozen, extra_large_half_dozen, and extra_large_dozen columns into one “Size” column and one “Quantity” column.\n\n\n  unite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep=\"arge_\",\n               values_to = \"Earnings\")\n\n\n# A tibble: 480 × 4\n   Date          Size    Quantity   Earnings\n   <chr>         <chr>   <chr>         <dbl>\n 1 January_2004  l       half_dozen     126 \n 2 January_2004  l       dozen          230 \n 3 January_2004  extra_l half_dozen     132 \n 4 January_2004  extra_l dozen          230 \n 5 February_2004 l       half_dozen     128.\n 6 February_2004 l       dozen          226.\n 7 February_2004 extra_l half_dozen     134.\n 8 February_2004 extra_l dozen          230 \n 9 March_2004    l       half_dozen     131 \n10 March_2004    l       dozen          225 \n# … with 470 more rows\n\nAfter creating an “Egg Size” column and one “Eggs Per Carton” column, the “Egg Size” labels said “l” and “extra_l”, so I had to use mutate() to fix the labels.\n\n\nlibrary(tidyverse)\n  unite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) \n\n\n# A tibble: 480 × 4\n   Date          Size        Quantity   Earnings\n   <chr>         <chr>       <chr>         <dbl>\n 1 January_2004  Large       Half Dozen     126 \n 2 January_2004  Large       Dozen          230 \n 3 January_2004  Extra Large Half Dozen     132 \n 4 January_2004  Extra Large Dozen          230 \n 5 February_2004 Large       Half Dozen     128.\n 6 February_2004 Large       Dozen          226.\n 7 February_2004 Extra Large Half Dozen     134.\n 8 February_2004 Extra Large Dozen          230 \n 9 March_2004    Large       Half Dozen     131 \n10 March_2004    Large       Dozen          225 \n# … with 470 more rows\n\nAfter reformatting the data to show the correct number of variables, I decided to use filter(“Earnings” != 0) to remove any missing data.\n\n\nunite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) %>%\n  filter(\"Earnings\" != 0)\n\n\n# A tibble: 480 × 4\n   Date          Size        Quantity   Earnings\n   <chr>         <chr>       <chr>         <dbl>\n 1 January_2004  Large       Half Dozen     126 \n 2 January_2004  Large       Dozen          230 \n 3 January_2004  Extra Large Half Dozen     132 \n 4 January_2004  Extra Large Dozen          230 \n 5 February_2004 Large       Half Dozen     128.\n 6 February_2004 Large       Dozen          226.\n 7 February_2004 Extra Large Half Dozen     134.\n 8 February_2004 Extra Large Dozen          230 \n 9 March_2004    Large       Half Dozen     131 \n10 March_2004    Large       Dozen          225 \n# … with 470 more rows\n\nAfter reformatting my data, I decided to analyze how size impacts earnings. First, I filtered out extra large egg data so I could focus on finding the central tendencies of the large egg data.\n\n\nunite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) %>%\n  filter(Size != \"Extra Large\")\n\n\n# A tibble: 240 × 4\n   Date          Size  Quantity   Earnings\n   <chr>         <chr> <chr>         <dbl>\n 1 January_2004  Large Half Dozen     126 \n 2 January_2004  Large Dozen          230 \n 3 February_2004 Large Half Dozen     128.\n 4 February_2004 Large Dozen          226.\n 5 March_2004    Large Half Dozen     131 \n 6 March_2004    Large Dozen          225 \n 7 April_2004    Large Half Dozen     131 \n 8 April_2004    Large Dozen          225 \n 9 May_2004      Large Half Dozen     131 \n10 May_2004      Large Dozen          225 \n# … with 230 more rows\n\nAfter filtering out the extra large egg data, I used summarize() to find the central tendencies of large egg earnings.\n\n\nunite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) %>%\n  filter(Size != \"Extra Large\") %>%\n  summarize(\"Large Minimum\"=min(Earnings), \"Large Median\"=median(Earnings), \"Large Mean\"=mean(Earnings), \"Large Maximum\"=max(Earnings))\n\n\n# A tibble: 1 × 4\n  `Large Minimum` `Large Median` `Large Mean` `Large Maximum`\n            <dbl>          <dbl>        <dbl>           <dbl>\n1             126           202.         205.            278.\n\nTo find the central tendencies of extra large egg earnings, I copied and pasted the code from the previous step and replaced “Size != ‘Extra Large’” with “Size == ‘Extra Large’”.\n\n\nunite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) %>%\n  filter(Size == \"Extra Large\") %>%\n  summarize(\"Extra Large Minimum\"=min(Earnings), \"Extra Large Median\"=median(Earnings), \"Extra Large Mean\"=mean(Earnings), \"Extra Large Maximum\"=max(Earnings))\n\n\n# A tibble: 1 × 4\n  `Extra Large Min… `Extra Large Me… `Extra Large Me… `Extra Large Ma…\n              <dbl>            <dbl>            <dbl>            <dbl>\n1               132             209.             216.              290\n\nAfter finding the central tendencies of the large vs extra large egg earnings, I made a box plot to provide a visual representation of the central tendencies.\n\n\nlibrary(tidyverse)\nunite(eggs_data, \"Date\", month:year) %>%\n  pivot_longer(cols=large_half_dozen:extra_large_dozen,\n               names_to = c(\"Size\", \"Quantity\"),\n               names_sep = \"arge_\",\n               values_to = \"Earnings\") %>%\n  mutate(Size = case_when(\n    Size == 'l'~\"Large\",\n    Size == 'extra_l'~ \"Extra Large\")) %>%\n  mutate(Quantity=case_when(\n    Quantity == \"half_dozen\" ~ \"Half Dozen\",\n    Quantity == \"dozen\" ~ \"Dozen\")) %>%\n  ggplot(aes(Size, Earnings)) +\n  geom_boxplot() + labs(title = \"Extra Large vs Large Egg Earnings\")\n\n\n\n\nAccording to the boxplot above, the extra large egg earnings were slightly higher than the large egg earnings, but not by a lot.\n\n\n\n",
    "preview": "posts/2021-08-18-rhyslongeggs/rhyslongeggs_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-08-24T08:21:48-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-10-cars/",
    "title": "iris",
    "description": "iris dataset| ErinTracy",
    "author": [],
    "date": "2021-08-10",
    "categories": [
      "-homework 3"
    ],
    "contents": "\nColumn 1\nWidth\n\n\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = 0.1)\n\n\n\n\nlength\n\n\nggplot(iris, aes(Petal.Length)) + geom_bar()\n\n\n\n\nSpecies\n\n\nggplot(iris, aes(Species)) + geom_bar()\n\n\n\n\nColumn 2\nThe largest iris\n\n\niris %>% \n  arrange(desc(Petal.Length)) %>% \n  head(100) %>% \n  select(Petal.Length, Petal.Width, Species) %>% \n  DT::datatable()\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\"],[6.9,6.7,6.7,6.6,6.4,6.3,6.1,6.1,6.1,6,6,5.9,5.9,5.8,5.8,5.8,5.7,5.7,5.7,5.6,5.6,5.6,5.6,5.6,5.6,5.5,5.5,5.5,5.4,5.4,5.3,5.3,5.2,5.2,5.1,5.1,5.1,5.1,5.1,5.1,5.1,5.1,5,5,5,5,4.9,4.9,4.9,4.9,4.9,4.8,4.8,4.8,4.8,4.7,4.7,4.7,4.7,4.7,4.6,4.6,4.6,4.5,4.5,4.5,4.5,4.5,4.5,4.5,4.5,4.4,4.4,4.4,4.4,4.3,4.3,4.2,4.2,4.2,4.2,4.1,4.1,4.1,4,4,4,4,4,3.9,3.9,3.9,3.8,3.7,3.6,3.5,3.5,3.3,3.3,3],[2.3,2.2,2,2.1,2,1.8,2.5,1.9,2.3,2.5,1.8,2.1,2.3,2.2,1.8,1.6,2.3,2.1,2.5,1.8,2.1,2.2,1.4,2.4,2.4,2.1,1.8,1.8,2.1,2.3,1.9,2.3,2.3,2,1.6,1.9,2,2.4,1.5,2.3,1.9,1.8,1.7,2,1.5,1.9,1.5,1.5,2,1.8,1.8,1.8,1.4,1.8,1.8,1.4,1.6,1.4,1.2,1.5,1.5,1.3,1.4,1.5,1.3,1.5,1.5,1.5,1.5,1.6,1.7,1.4,1.4,1.3,1.2,1.3,1.3,1.5,1.3,1.2,1.3,1,1.3,1.3,1.3,1,1.3,1.3,1.2,1.4,1.1,1.2,1.1,1,1.3,1,1,1,1,1.1],[\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"versicolor\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"versicolor\",\"virginica\",\"virginica\",\"virginica\",\"versicolor\",\"versicolor\",\"virginica\",\"virginica\",\"virginica\",\"versicolor\",\"versicolor\",\"virginica\",\"virginica\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"virginica\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Petal.Length<\\/th>\\n      <th>Petal.Width<\\/th>\\n      <th>Species<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n\n\n\n",
    "preview": "posts/2021-08-10-cars/cars_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-24T08:18:54-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 320
  },
  {
    "path": "posts/2021-08-10-hello-world/",
    "title": "Hello World",
    "description": "Marina's first attempt at using distill",
    "author": [
      {
        "name": "Marina",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nTime to read in data. Start with an easy one\n\n\n\n\n# A tibble: 6 × 32\n  hotel        is_canceled lead_time arrival_date_ye… arrival_date_mo…\n  <chr>              <dbl>     <dbl>            <dbl> <chr>           \n1 Resort Hotel           0       342             2015 July            \n2 Resort Hotel           0       737             2015 July            \n3 Resort Hotel           0         7             2015 July            \n4 Resort Hotel           0        13             2015 July            \n5 Resort Hotel           0        14             2015 July            \n6 Resort Hotel           0        14             2015 July            \n# … with 27 more variables: arrival_date_week_number <dbl>,\n#   arrival_date_day_of_month <dbl>, stays_in_weekend_nights <dbl>,\n#   stays_in_week_nights <dbl>, adults <dbl>, children <dbl>,\n#   babies <dbl>, meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>, …\n\nTry a harder one: an excel file with a number of useless rows\n\n# A tibble: 6 × 17\n  blank pay_grade SWOC_male SWOC_female SWOC_total SWC_male SWC_female\n  <chr> <chr>         <dbl>       <dbl>      <dbl>    <dbl>      <dbl>\n1 <NA>  E-1           31229        5717      36946      563        122\n2 <NA>  E-2           53094        8388      61482     1457        275\n3 <NA>  E-3          131091       21019     152110     4264       1920\n4 <NA>  E-4          112710       16381     129091     9491       4662\n5 <NA>  E-5           57989       11021      69010    10937       6576\n6 <NA>  E-6           19125        4654      23779    10369       4962\n# … with 10 more variables: SWC_total <dbl>, JSM_male <dbl>,\n#   JSM_female <dbl>, JSM_total <dbl>, CM_male <dbl>,\n#   CM_female <dbl>, CM_total <dbl>, Total_Male <dbl>,\n#   Total_Female <dbl>, Total_Total <dbl>\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:01-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-homework-1/",
    "title": "Homework 1",
    "description": "Homework 1",
    "author": [],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nHello, my name is Shih-Yen Pan. I am a PhD student in Economics at UMass, Amherst.\nIn this post, I will introduce the Two Sum problem from LeetCode and a solution using double for loop provided by Pascal Schmidt here. I thought it might be useful to see what for loop looks like in R.\nThe idea of the problem is, given a list of integers nums and an integer target, to write a function that spits out, if they exist, two different integers in nums that sum up to the target integer.\ntwo_sum <- function(nums, target) {\n\n  for(i in seq_along(nums)) {\n    for(j in seq_along(nums)[-length(nums)]) {\n      \n      sum <- nums[i] + nums[j + 1]\n      if(sum == target) {\n        \n        first <- i\n        second <- j + 1\n        output <- c(nums[first], nums[second])\n        return(output)\n        \n      }\n      \n    }\n    \n  }\n  \n}\nThis is what the function two_sum is doing: Starting from the beginning of the vector ‘nums’, for each integer, search the rest of vector using the second for loop. If the two numbers sum up to the target integer, the code returns those two numbers; otherwise, continue doing the same with the next integer in the list.\nExample:\nnums <- c(1, 2, 3, 5, 6, 9, 11)\ntarget <- 9\ntwo_sum(nums, target)\nThis should return 3 and 6.\nTry it out yourself with different numbers! How can the code be modified to return all the different pairs that sum to the target integer?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:04-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-hw-1-bakharia/",
    "title": "iris",
    "description": "Here is the iris dataset.",
    "author": [
      {
        "name": "bakharia",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "homework 1",
      "iris",
      "bakharia"
    ],
    "contents": "\nHere is a plot of the iris dataset:\n\n\nlibrary(datasets)\nplot(iris)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-10-hw-1-bakharia/iris_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-24T08:19:07-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-10-hw01/",
    "title": "HW01",
    "description": "A short description of the post.",
    "author": [],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:10-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-iris/",
    "title": "iris",
    "description": "Here is the iris dataset.",
    "author": [
      {
        "name": "bakharia",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "homework 3",
      "iris",
      "bakharia"
    ],
    "contents": "\nHere is a plot of the iris dataset:\n\n\nlibrary(datasets)\nplot(iris)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-10-iris/iris_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-24T08:19:14-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-10-meet-rhys/",
    "title": "Meet Rhys",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Rhys Long",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "homework one",
      "introduce yourself",
      "rhys long"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:16-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-michelle-manning/",
    "title": "Michelle Manning",
    "description": "Intro for me.",
    "author": [
      {
        "name": "Michelle Manning",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\n\n\nlibrary(dslabs)\ndata(\"gapminder\")\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\n\ngapminder %>% \n  select(continent, region, gdp, population) %>%\n  mutate(gdp_per_capita = gdp / population) %>% \n  arrange(desc(gdp_per_capita)) %>%\n  ggplot(aes(x=gdp_per_capita)) + \n  geom_histogram(binwidth = 30)\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-10-michelle-manning/michelle-manning_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-24T08:19:20-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-10-noahblogpost/",
    "title": "Noahblogpost",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\n\n\n8+8\n\n\n[1] 16\n\n\n\n8==2\n\n\n[1] FALSE\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:23-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-test/",
    "title": "TEST",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "bakharia",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:27-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-title/",
    "title": "title",
    "description": "A short description of the post.",
    "author": [],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:29-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-zoes-post/",
    "title": "Zoe's Post",
    "description": "my first post",
    "author": [
      {
        "name": "Zoe Bean",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\nHi! I’m Zoe. I really like for loops. Here’s one in R!\n\n\ncool_sequence<-c(1,1,2,3,5,8,13,21,34)\ntotal<-0\nfor (el in cool_sequence) {\n  total=total+el\n}\nprint(total)\n\n\n[1] 88\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:19:31-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-07-still-learning-githib-workflow/",
    "title": "Still Learning the GitHib Workflow",
    "description": "Fingers crossed this post works",
    "author": [
      {
        "name": "Maddi Hertz",
        "url": {}
      }
    ],
    "date": "2021-08-07",
    "categories": [],
    "contents": "\nUsing Markdown is fun!\nI learned a lot this summer — Markdown is your friend, as are flexdashboard and Stack Overflow.\nAlso, always keep in mind that social scientists understand data and data management just as well (if not better) than the computer scientists.\nTidyTuesday\nNew goal: Participate in Tidy Tuesdays\nExample of R Chunk\n\n\n2+2\n\n\n[1] 4\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:18:48-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-07-welcome/",
    "title": "Welcome to DACSS 601",
    "description": "August 2021 Orientation Session",
    "author": [
      {
        "name": "Meredith Rolfe",
        "url": "http://umass.edu/sbs/dacss"
      }
    ],
    "date": "2021-08-06",
    "categories": [
      "welcome"
    ],
    "contents": "\nWelcome to the Data Analytics and Computational Social Science Programs’ Orientation 2021 session of DACSS 601 Foundations of Data Science. This blog will feature the work of our incoming M.S. students taking the class, as well as PhD students in the College of Behavioral and Social Sciences who have decided to participate through “R Bootcamp”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-24T08:18:50-04:00",
    "input_file": {}
  }
]
